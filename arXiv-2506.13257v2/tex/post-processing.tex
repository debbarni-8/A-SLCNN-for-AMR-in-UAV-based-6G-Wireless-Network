\section{Post-processing the posteriors}\label{sec:savs}
%
Although horseshoe priors on the quantile invariant and difference vectors shrink toward sparsity, exact sparsity cannot be achieved in finite samples due to absolute continuity of the prior distributions \citep{carvalho_handling_2009}. We suggest post-processing the posterior with a thresholding algorithm motivated from decision theory \citep{lindley1968choice} to project the posterior onto a possibly sparse subset.\footnote{Simply calculating Bayesian p-values based on the posteriors for $\beta_0$ and $\boldsymbol{\sigma}$ might be misleading due the effect of correlation in the posteriors as well as potential multi-modality} On the one hand, this helps intuitively assessing the marginal importance of quantile invariant as well as quantile variant effects. \citet{piironen_sparsity_2017} show that post-estimation projection may also reduce variance in variable selection. On the other, it can sharpen inference of true zero effects, as well as lead to improved predictions \citep{huber2021inducing,kohns2025flexible}. This literature has renewed attention with \citet{hahn2015decoupling} for normal linear models which has been extended to Bayesian quantile regression in \citet{kohns2021decoupling}, and \citet{feldman2023bayesian}. 
%

% \beta_0
Define the linear predictor of interest for the quantile invariant effect as $X\beta_0$, then a possibly sparse vector, $\xi^0$, can be found by solving:
%
\begin{equation} \label{eq:sparse_beta_0}
\overline{\xi}^0 = \underset{\xi^0}{\text{argmin}} \; \frac{1}{2}\norm{X\beta_0 - X\xi^0}^2_2 + \sum^{K}_{j=1}\upsilon^0_j|\xi^0_{j}|,
\end{equation}
%
where $\upsilon^0_j$ is an adaptive penalty factor akin to adaptive lasso \citep{zou2006adaptive}. Likewise, define the linear predictor that captures quantile variation as $\boldsymbol{\tilde{X}}\boldsymbol{\sigma}$, then a possibly sparse vector, $\boldsymbol{\xi^{\sigma}}$, can be found by solving:
%
\begin{equation} \label{eq:sparse_sigma}
    \overline{\xi}^{\boldsymbol{\sigma}} = \underset{\boldsymbol{\xi_{\sigma}}}{\text{argmin}} \; \frac{1}{2}\norm{\boldsymbol{\tilde{X}}\boldsymbol{\sigma} - \boldsymbol{\tilde{X}}\xi^{\boldsymbol{\sigma}}}^2_2 + \sum^{Q}_{q=1}\sum^{K}_{j=1}\upsilon^{\sigma}_{q,j}|\xi^{\boldsymbol{\sigma}}_{q,j}|.
\end{equation}
%
These projections can be solved for each MCMC draw $s = 1,\dotsc,S$ in order to retrieve pseudo model-average posteriors \citep{bhattacharya2016fast}. The level of sparsity in Equations~\ref{eq:sparse_beta_0} and~\ref{eq:sparse_sigma} is determined by the penalties $(\upsilon^0,\upsilon^{\sigma})$. For computational convenience, the penalty term is set inversely proportional to the posterior draw of the coefficient following \citet{ray2018signal}. We maintain the naming of the authors in describing this sparsification as algorithm as the $\mathrm{SAVS}$ algorithm. We refer to the $\NCQVP$ with $\mathrm{SAVS}$ algorithm applied as the $\NCQVP_\mathrm{SAVS}$ model.
