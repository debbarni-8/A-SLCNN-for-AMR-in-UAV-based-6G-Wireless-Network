{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlrGuXP6PuZ7Fj8KrUVsQz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5EHqn6wQRg2o"},"outputs":[],"source":["import os\n","from keras.models import Model\n","from keras.layers import Input, Dense, ReLU, Dropout, Softmax, Conv2D, MaxPool2D, Lambda, GaussianNoise\n","from keras.layers import Bidirectional, Flatten, CuDNNGRU\n","from keras.utils.vis_utils import plot_model\n","\n","# Define ICAMC model (custom CNN architecture)\n","def ICAMC(weights=None,\n","          input_shape=[2,1024],\n","          classes=26,\n","          **kwargs):\n","\n","    # Check if weights are valid\n","    if weights is not None and not (os.path.exists(weights)):\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization), '\n","                         'or the path to the weights file to be loaded.')\n","\n","    dr = 0.45   # Dropout rate for regularizationm [0.3, 0.35, 0.4, 0.45, 0.5]\n","\n","    # Input layer\n","    input = Input(input_shape + [1], name='input')\n","\n","    # First convolution layer + pooling\n","    x = Conv2D(32, (1, 8), activation=\"relu\", name=\"conv1\",\n","               padding='same', kernel_initializer='glorot_uniform')(input)\n","    x = MaxPool2D(pool_size=(2, 2))(x)\n","\n","    # Second convolution layer\n","    x = Conv2D(32, (1, 4), activation=\"relu\", name=\"conv2\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","\n","    # Third convolution layer\n","    x = Conv2D(64, (1, 8), activation=\"relu\", name=\"conv3\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","    x = Dropout(dr)(x)   # Dropout for regularization\n","\n","    # Fourth convolution layer\n","    x = Conv2D(64, (1, 8), activation=\"relu\", name=\"conv4\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","    x = Dropout(dr)(x)   # Dropout again\n","\n","    # Flatten convolution outputs for dense layers\n","    x = Flatten()(x)\n","\n","    # Fully connected dense layer\n","    x = Dense(64, activation='relu', name='dense1')(x)\n","    x = Dropout(dr)(x)   # Dropout\n","\n","    # Add Gaussian noise for robustness against overfitting\n","    x = GaussianNoise(1)(x)\n","\n","    # Final classification layer (softmax for multiclass output)\n","    x = Dense(classes, activation='softmax', name='dense2')(x)\n","\n","    # Build model\n","    model = Model(inputs=input, outputs=x)\n","\n","    # Load pre-trained weights if provided\n","    if weights is not None:\n","        model.load_weights(weights)\n","\n","    return model\n","\n","\n","# Run only when executed directly\n","import keras\n","if __name__ == '__main__':\n","    # Create the model\n","    model = ICAMC(None, input_shape=[2,1024], classes=26)\n","\n","    # Compile with Adam optimizer & categorical crossentropy loss\n","    adam = keras.optimizers.Adam(\n","        learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n","        epsilon=None, decay=0.0, amsgrad=False\n","    )  #learning_rate= [0.0001, 0.0005, 0.001, 0.005, 0.01]\n","    model.compile(loss='categorical_crossentropy',\n","                  metrics=['accuracy'],\n","                  optimizer=adam)\n","\n","    # Print model details\n","    print('Model layers:', model.layers)        # list of layers\n","    print('Model config:', model.get_config())  # config dictionary\n","    print('Model summary:')\n","    print(model.summary())                      # detailed summary"]},{"cell_type":"code","source":["import matplotlib\n","matplotlib.use('TkAgg')   # Use TkAgg backend for matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import csv\n","import itertools\n","from sklearn.metrics import precision_recall_fscore_support, mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n","\n","\n","# =======================\n","# Plot confusion matrix\n","# =======================\n","def plot_confusion_matrix(cm, title='', cmap=plt.get_cmap(\"Blues\"), labels=[], save_filename=None):\n","    plt.figure(figsize=(10, 7))\n","    plt.imshow(cm*100, interpolation='nearest', cmap=cmap)  # Multiply by 100 for percentage scale\n","    plt.title(title, fontsize=10)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(labels))\n","    # Add class labels to x and y axis\n","    plt.xticks(tick_marks, labels, rotation=90, size=12)\n","    plt.yticks(tick_marks, labels, size=12)\n","\n","    # Add values to each cell\n","    for i in range(len(tick_marks)):\n","        for j in range(len(tick_marks)):\n","            if i != j:\n","                # Off-diagonal entries: prediction errors\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10)\n","            else:\n","                # Diagonal entries: correct predictions (highlighted in orange)\n","                color = 'darkorange'\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10, color=color)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label', fontdict={'size':16,})\n","    plt.xlabel('Predicted label', fontdict={'size':16,})\n","\n","    # Save confusion matrix plot if filename provided\n","    if save_filename is not None:\n","        plt.savefig(save_filename, format='pdf', dpi=1200, bbox_inches='tight')\n","    plt.close()\n","\n","# =======================\n","# Calculate confusion matrix\n","# =======================\n","def calculate_confusion_matrix(Y, Y_hat, classes):\n","    n_classes = len(classes)\n","    conf = np.zeros([n_classes, n_classes])      # Raw confusion matrix\n","    confnorm = np.zeros([n_classes, n_classes])  # Normalized confusion matrix\n","\n","    # Fill confusion matrix by comparing true vs predicted labels\n","    for k in range(0, Y.shape[0]):\n","        i = list(Y[k,:]).index(1)           # True class index\n","        j = int(np.argmax(Y_hat[k,:]))      # Predicted class index\n","        conf[i,j] = conf[i,j] + 1\n","\n","    # Normalize rows (per-class accuracy)\n","    for i in range(0, n_classes):\n","        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n","\n","    # Count correct and incorrect predictions\n","    right = np.sum(np.diag(conf))\n","    wrong = np.sum(conf) - right\n","    return confnorm, right, wrong\n","\n","# =======================\n","# Calculate per-class accuracy at given SNR\n","# =======================\n","def calculate_acc_at1snr_from_cm(cm):\n","    return np.round(np.diag(cm) / np.sum(cm, axis=1), 3)\n","\n","# =======================\n","# Calculate metrics: Accuracy\n","# =======================\n","def calculate_metrics(Y, Y_hat):\n","    Y_true = np.argmax(Y, axis=1)       # Convert one-hot to label index\n","    Y_pred = np.argmax(Y_hat, axis=1)\n","    accuracy = accuracy_score(Y_true, Y_pred)\n","\n","    return accuracy\n","\n","# =======================\n","# Calculate accuracy & metrics per SNR\n","# =======================\n","def calculate_acc_cm_each_snr(Y, Y_hat, Z, classes=None, save_figure=True, min_snr=0):\n","    Z_array = Z[:, 0]                # Extract SNR values\n","    snrs = sorted(list(set(Z_array)))  # Unique SNRs\n","    acc = np.zeros(len(snrs))          # Store overall accuracy per SNR\n","    acc_mod_snr = np.zeros((len(classes), len(snrs)))  # Store per-class accuracy\n","\n","    # Dictionary to store metrics at each SNR\n","    metrics = {\n","        'accuracy': []\n","    }\n","\n","    i = 0\n","    for snr in snrs:\n","        # Select data corresponding to this SNR\n","        Y_snr = Y[np.where(Z_array == snr)]\n","        Y_hat_snr = Y_hat[np.where(Z_array == snr)]\n","\n","        # Compute confusion matrix\n","        cm, right, wrong = calculate_confusion_matrix(Y_snr, Y_hat_snr, classes)\n","        accuracy = calculate_metrics(Y_snr, Y_hat_snr)\n","\n","        # Store metrics\n","        metrics['accuracy'].append(accuracy)\n","\n","\n","        # Plot confusion matrix if above threshold SNR\n","        if snr >= min_snr:\n","            plot_confusion_matrix(cm, cmap=plt.cm.Blues, labels=classes, save_filename='figure/cm_snr{}.pdf'.format(snr))\n","\n","        # Overall accuracy for this SNR\n","        acc[i] = round(1.0 * right / (right + wrong), 3)\n","        print('Accuracy at %ddb: %.2f%s / (%d + %d)' % (snr, 100*acc[i], '%', right, wrong))\n","        acc_mod_snr[:, i] = calculate_acc_at1snr_from_cm(cm)\n","        i += 1\n","\n","    # Save accuracy results into a file\n","    fd = open('acc_overall_128k_on_512k_wts.dat', 'wb')\n","    pickle.dump(('128k', '512k', acc), fd)\n","    fd.close()\n","\n","    # Plot Accuracy\n","    for metric, values in metrics.items():\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(snrs, values, label=metric)\n","        for x, y in zip(snrs, values):\n","            plt.text(x, y, round(y, 3), ha='center', va='bottom', fontsize=8)\n","        plt.xlabel(\"Signal to Noise Ratio\")\n","        plt.ylabel(metric.capitalize())\n","        plt.title(f\"{metric.capitalize()} vs SNR\")\n","        plt.legend()\n","        plt.grid()\n","        plt.savefig(f'figure/{metric}_vs_snr.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n","        plt.show()"],"metadata":{"id":"Q6PH4BGjR7MI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","\n","# Set Keras backend to TensorFlow\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Select which GPU to use (here GPU 0 is visible, others hidden)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","# Imports for plotting colored line segments\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","\n","import sys, h5py\n","import tensorflow as tf\n","import pandas as pd\n","\n","# Utility function from Keras to convert labels to one-hot encoding\n","from keras.utils.np_utils import to_categorical\n","\n","# List of modulation classes used in the dataset (26 total)\n","classes = [\n","    'BPSK',        # Binary Phase Shift Keying\n","    'QPSK',        # Quadrature Phase Shift Keying\n","    '8PSK',        # 8-level Phase Shift Keying\n","    '16PSK',       # 16-level Phase Shift Keying\n","    '32PSK',       # 32-level Phase Shift Keying\n","    '64PSK',       # 64-level Phase Shift Keying\n","    '4QAM',        # 4-level Quadrature Amplitude Modulation\n","    '8QAM',        # 8-level Quadrature Amplitude Modulation\n","    '16QAM',       # 16-level Quadrature Amplitude Modulation\n","    '32QAM',       # 32-level Quadrature Amplitude Modulation\n","    '64QAM',       # 64-level Quadrature Amplitude Modulation\n","    '128QAM',      # 128-level Quadrature Amplitude Modulation\n","    '256QAM',      # 256-level Quadrature Amplitude Modulation\n","    '2FSK',        # 2-level Frequency Shift Keying\n","    '4FSK',        # 4-level Frequency Shift Keying\n","    '8FSK',        # 8-level Frequency Shift Keying\n","    '16FSK',       # 16-level Frequency Shift Keying\n","    '4PAM',        # 4-level Pulse Amplitude Modulation\n","    '8PAM',        # 8-level Pulse Amplitude Modulation\n","    '16PAM',       # 16-level Pulse Amplitude Modulation\n","    'AM-DSB',      # Amplitude Modulation - Double Sideband\n","    'AM-DSB-SC',   # Amplitude Modulation - Double Sideband Suppressed Carrier\n","    'AM-USB',      # Amplitude Modulation - Upper Sideband\n","    'AM-LSB',      # Amplitude Modulation - Lower Sideband\n","    'FM',          # Frequency Modulation\n","    'PM'           # Phase Modulation\n","]"],"metadata":{"id":"BBFoDfqDSeRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load training and testing data\n","\n","\n","\n","# Open training dataset file (.mat format) with h5py\n","data1 = h5py.File('Dataset/HisarMod2019.1/Train/train.mat', 'r')\n","# Extract data array stored under key 'data_save'\n","train = data1['data_save'][:]\n","# Rearrange dimensions: move axis 0 to the end (needed for Keras/TensorFlow input format)\n","train = train.swapaxes(0, 2)\n","\n","# Open testing dataset file (.mat format)\n","data2 = h5py.File('Dataset/HisarMod2019.1/Test/test.mat', 'r')\n","# Extract test data\n","test = data2['data_save'][:]\n","# Rearrange dimensions for consistency\n","test = test.swapaxes(0, 2)\n","\n","# Add a channel dimension at the end (axis=3)\n","# This converts data into shape (samples, height, width, channels),\n","# which is the standard input format for Conv2D in Keras\n","train = np.expand_dims(train, axis=3)\n","test = np.expand_dims(test, axis=3)"],"metadata":{"id":"ybOeAdmhSkcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load and preprocess labels for training and testing data\n","\n","# -------------------------\n","# Training labels\n","# -------------------------\n","# Read CSV file containing training labels (no header in CSV)\n","train_labels = pd.read_csv('Dataset/HisarMod2019.1/Train/train_labels1.csv', header=None)\n","# Convert pandas DataFrame to numpy array\n","train_labels = np.array(train_labels)\n","# Convert labels to one-hot encoding for classification\n","# Example: label '3' becomes [0,0,0,1,0,...]\n","train_labels = to_categorical(train_labels, num_classes=None)\n","\n","# -------------------------\n","# Testing labels\n","# -------------------------\n","# Read CSV file containing testing labels\n","test_labels = pd.read_csv('Dataset/HisarMod2019.1/Test/test_labels1.csv', header=None)\n","# Convert to numpy array\n","test_labels = np.array(test_labels)\n","# Convert testing labels to one-hot encoding\n","test_labels = to_categorical(test_labels, num_classes=None)"],"metadata":{"id":"qW3nMIR3SwYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load Signal-to-Noise Ratio (SNR) values for training and testing data\n","\n","# -------------------------\n","# Training SNR\n","# -------------------------\n","# Read CSV file containing SNR values for each training sample (no header in CSV)\n","train_snr = pd.read_csv('Dataset/HisarMod2019.1/Train/train_snr.csv', header=None)\n","# Convert pandas DataFrame to numpy array for easier manipulation\n","train_snr = np.array(train_snr)\n","\n","# -------------------------\n","# Testing SNR\n","# -------------------------\n","# Read CSV file containing SNR values for each testing sample\n","test_snr = pd.read_csv('Dataset/HisarMod2019.1/Test/test_snr.csv', header=None)\n","# Convert to numpy array\n","test_snr = np.array(test_snr)"],"metadata":{"id":"0AwPhnZMTJgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Split dataset into training, validation, and testing sets\n","# ===========================\n","\n","# Total number of examples in the training data\n","n_examples = train.shape[0]\n","\n","# Define number of training samples (60% of total)\n","n_train = int(n_examples * 0.6)\n","\n","# Define number of validation samples (15% of total)\n","n_val = int(n_examples * 0.15)\n","\n","# Randomly select indices for training samples without replacement\n","train_idx = list(np.random.choice(range(0, n_examples), size=n_train, replace=False))\n","\n","# Remaining indices are used for validation\n","val_idx = list(set(range(0, n_examples)) - set(train_idx))\n","\n","# Shuffle the indices to ensure randomness\n","np.random.shuffle(train_idx)\n","np.random.shuffle(val_idx)\n","\n","# -------------------------\n","# Prepare training and validation sets\n","# -------------------------\n","X_train = train[train_idx]           # Training data samples\n","Y_train = train_labels[train_idx]    # Corresponding one-hot training labels\n","\n","X_val = train[val_idx]               # Validation data samples\n","Y_val = train_labels[val_idx]        # Corresponding one-hot validation labels\n","\n","# -------------------------\n","# Testing set\n","# -------------------------\n","X_test = test                        # Test data samples\n","Y_test = test_labels                 # Test labels (one-hot encoded)\n","Z_test = test_snr                     # SNR values for each test sample"],"metadata":{"id":"smgOu26mTNkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up some params\n","nb_epoch = 200     # number of epochs to train on\n","batch_size = 300  # training batch size"],"metadata":{"id":"9yLT3r4DTW6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Initialize and compile the model\n","# ===========================\n","\n","# Create an instance of the ICAMC model\n","model = ICAMC()\n","\n","# Compile the model\n","# - Loss: 'categorical_crossentropy' (for multi-class classification)\n","# - Metrics: 'accuracy' to monitor performance\n","# - Optimizer: 'adam', a popular adaptive optimizer\n","model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","\n","# Plot the model architecture and save it as an image\n","# - 'show_shapes=True' displays the input/output shape for each layer\n","plot_model(model, to_file='model.png', show_shapes=True)  # Saves model diagram to 'model.png'\n","\n","# Print a detailed summary of the model to console\n","# - Shows layer types, output shapes, and number of parameters\n","model.summary()"],"metadata":{"id":"H5pAnhAqTa1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Define file path to save model weights\n","# ===========================\n","filepath = 'weights/weights.h5'  # Path where trained model weights will be saved\n","\n","# ===========================\n","# Record the start time for training\n","# ===========================\n","import time\n","TRS_PROPOSED = time.time()  # Store current time (seconds since epoch) to measure training duration later"],"metadata":{"id":"RPbpPiBdTedO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Train the model\n","# ===========================\n","\n","history = model.fit(\n","    X_train,                 # Training data inputs\n","    Y_train,                 # Training data labels (one-hot encoded)\n","    batch_size=batch_size,   # Number of samples per gradient update\n","    epochs=nb_epoch,         # Number of complete passes through the training dataset\n","    verbose=2,               # Verbosity mode (2 = one line per epoch)\n","    validation_data=(X_val, Y_val),  # Data for validation at the end of each epoch\n","    callbacks=[              # List of callbacks to apply during training\n","        # --------------------------\n","        # Save the best model weights based on validation loss\n","        # --------------------------\n","        keras.callbacks.ModelCheckpoint(\n","            filepath,                 # File path to save the model weights\n","            monitor='val_loss',        # Metric to monitor (validation loss)\n","            verbose=1,                 # Print messages when saving\n","            save_best_only=True,       # Only save the model if the monitored metric improves\n","            mode='auto'                # Let Keras decide whether to minimize or maximize the monitored metric\n","        ),\n","        # --------------------------\n","        # Reduce learning rate when validation loss plateaus\n","        # --------------------------\n","        keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_loss',       # Metric to monitor\n","            factor=0.5,               # Factor to reduce the learning rate by\n","            verbose=1,                # Print messages when reducing LR\n","            patince=5,                # Number of epochs with no improvement before reducing LR (note: should be 'patience', check typo)\n","            min_lr=0.0000001          # Minimum learning rate\n","        ),\n","        # --------------------------\n","        # Early stopping to prevent overfitting\n","        # --------------------------\n","        keras.callbacks.EarlyStopping(\n","            monitor='val_loss',       # Metric to monitor\n","            patience=5,               # Stop training if no improvement after 5 epochs\n","            verbose=1,                # Print message when stopping\n","            mode='auto'               # Let Keras decide whether to minimize or maximize the monitored metric\n","        )\n","    ]\n",")"],"metadata":{"id":"_83BlNanTihN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Record the end time and calculate total training duration\n","# ===========================\n","\n","TRE_PROPOSED = time.time()           # Capture the current time after training finishes\n","T_PROPOSED = TRE_PROPOSED - TRS_PROPOSED  # Calculate total training time (seconds) by subtracting start time\n","T_PROPOSED                           # Display or store the total training duration"],"metadata":{"id":"ms1S55M0TtUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Evaluate the trained model on the test set\n","# ===========================\n","\n","TES_PROPOSED = time.time()  # Record the current time before evaluation (optional, for timing)\n","\n","# Evaluate the model on test data\n","# - Returns a list: [loss, accuracy] (because we compiled the model with 'accuracy' metric)\n","score = model.evaluate(\n","    X_test,            # Test data inputs\n","    Y_test,            # Test labels (one-hot encoded)\n","    verbose=1,         # Print progress bar for evaluation\n","    batch_size=batch_size  # Number of samples per evaluation step\n",")\n","\n","# Print evaluation results: loss and accuracy on test set\n","print(score)"],"metadata":{"id":"gLx2F55gUSFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_history(history) # plot loss curve"],"metadata":{"id":"YRiuseqsIPCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model):\n","    # ===========================\n","    # Load the best-trained model weights\n","    # ===========================\n","    model.load_weights(filepath)  # Load weights saved during training from 'weights/weights.h5'\n","\n","    # ===========================\n","    # Predict labels for the test set\n","    # ===========================\n","    test_Y_hat = model.predict(X_test, batch_size=batch_size)\n","    # 'test_Y_hat' contains predicted probabilities for each class (one-hot-like format)\n","\n","    # ===========================\n","    # Compute confusion matrix and overall accuracy\n","    # ===========================\n","    cm, right, wrong = calculate_confusion_matrix(Y_test, test_Y_hat, classes)\n","    # cm: normalized confusion matrix\n","    # right: number of correct predictions\n","    # wrong: number of incorrect predictions\n","\n","    acc = round(1.0 * right / (right + wrong), 4)  # Overall accuracy (0-1 scale)\n","    print('Overall Accuracy: %.2f%s / (%d + %d)' % (100 * acc, '%', right, wrong))\n","    # Prints accuracy as a percentage and counts of correct/incorrect predictions\n","\n","    # ===========================\n","    # Plot confusion matrix\n","    # ===========================\n","    plot_confusion_matrix(\n","        cm,\n","        labels=['BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '64PSK',\n","                '4QAM', '8QAM', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM',\n","                '2FSK', '4FSK', '8FSK', '16FSK',\n","                '4PAM', '8PAM', '16PAM',\n","                'AM-DSB', 'AM-DSB-SC', 'AM-USB', 'AM-LSB', 'FM', 'PM'],\n","        save_filename='figure/total_confusion.png'\n","    )\n","    # Saves the confusion matrix as a PNG file for visualization\n","\n","    # ===========================\n","    # Calculate accuracy for each SNR and plot metrics vs SNR\n","    # ===========================\n","    calculate_acc_cm_each_snr(\n","        Y_test,          # True labels\n","        test_Y_hat,      # Predicted labels\n","        Z_test,          # SNR values for each test sample\n","        classes,         # List of modulation classes\n","        min_snr=-18      # Minimum SNR value to plot\n","    )\n","    # - Plots overall accuracy, precision, recall, and F1-score vs SNR"],"metadata":{"id":"X9BJfQoiq6_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model)  #Computes confusion matrices for different SNR levels"],"metadata":{"id":"laOlXt4Mrpuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Record the end time of testing and calculate total testing duration\n","# ===========================\n","\n","TEE_PROPOSED = time.time()           # Capture the current time after test evaluation and prediction\n","TE_PROPOSED = TEE_PROPOSED - TES_PROPOSED  # Calculate total testing time in seconds"],"metadata":{"id":"1BBUAMOOxV7Q"},"execution_count":null,"outputs":[]}]}