{"cells":[{"cell_type":"code","execution_count":null,"id":"ecb5052a","metadata":{"id":"ecb5052a"},"outputs":[],"source":["import os\n","from keras.models import Model\n","from keras.layers import Input, Dense, ReLU, Dropout, Activation, concatenate, Softmax, Conv2D, MaxPool2D, Add, BatchNormalization\n","from keras.layers import Bidirectional, Flatten, CuDNNGRU\n","from keras.utils.vis_utils import plot_model\n","import keras\n","\n","# ===========================\n","# Define a custom ResNet-like CNN model\n","# ===========================\n","def ResNet(weights=None,\n","           input_shape=[2,1024],   # Input shape (e.g., 2x1024)\n","           classes=26,             # Number of output classes for classification\n","           **kwargs):\n","\n","    # Check if weights file exists if provided\n","    if weights is not None and not (os.path.exists(weights)):\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization), '\n","                         'or the path to the weights file to be loaded.')\n","\n","    dr = 0.6  # Dropout rate for regularization\n","\n","    # Input layer\n","    input = Input(input_shape + [1], name='input')  # Add channel dimension for Conv2D\n","\n","    # --------------------------\n","    # First convolutional layer\n","    # --------------------------\n","    x = Conv2D(256, (1,3), name=\"conv1\", kernel_initializer='glorot_uniform', padding='same')(input)\n","    x = Activation('relu')(x)\n","    # x = Dropout(dr)(x)  # Dropout optional, commented out\n","\n","    # --------------------------\n","    # Second convolutional layer\n","    # --------------------------\n","    x = Conv2D(256, (2,3), name=\"conv2\", kernel_initializer='glorot_uniform', padding='same')(x)\n","    # x = Dropout(dr)(x)  # Dropout optional\n","\n","    # --------------------------\n","    # Residual connection: add input to conv output\n","    # --------------------------\n","    x1 = Add()([input, x])\n","    x1 = Activation('relu')(x1)\n","\n","    # --------------------------\n","    # Third and fourth convolutional layers\n","    # --------------------------\n","    x = Conv2D(80, (1,3), activation=\"relu\", name=\"conv3\", kernel_initializer='glorot_uniform', padding='same')(x1)\n","    x = Conv2D(80, (1,3), activation=\"relu\", name=\"conv4\", kernel_initializer='glorot_uniform', padding='same')(x)\n","    x = Dropout(dr)(x)  # Apply dropout for regularization\n","\n","    # --------------------------\n","    # Flatten and fully connected layers\n","    # --------------------------\n","    x = Flatten()(x)\n","    x = Dense(128, activation='relu', name='fc1')(x)\n","    x = Dropout(dr)(x)\n","\n","    # --------------------------\n","    # Output layer with softmax activation for multi-class classification\n","    # --------------------------\n","    output = Dense(classes, activation='softmax', name='softmax')(x)\n","\n","    # Create Keras model\n","    model = Model(inputs=input, outputs=output)\n","\n","    # Load pre-trained weights if provided\n","    if weights is not None:\n","        model.load_weights(weights)\n","\n","    return model\n","\n","\n","# ===========================\n","# Main script to create, compile, and inspect the model\n","# ===========================\n","if __name__ == '__main__':\n","    # Instantiate the model with specific input and output size\n","    model = ResNet(None, input_shape=[2,128], classes=11)\n","\n","    # Define Adam optimizer with custom parameters\n","    adam = keras.optimizers.Adam(\n","        learning_rate=0.001,\n","        beta_1=0.9,\n","        beta_2=0.999,\n","        epsilon=None,\n","        decay=0.0,\n","        amsgrad=False\n","    )\n","\n","    # Compile the model\n","    # - Loss: categorical_crossentropy (for multi-class classification)\n","    # - Metrics: accuracy\n","    # - Optimizer: Adam\n","    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)\n","\n","    # Print model information\n","    print('Model layers:', model.layers)       # List of layers in the model\n","    print('Model config:', model.get_config()) # Configuration details of each layer\n","    print('Model summary:')\n","    model.summary()                            # Detailed summary with output shapes and parameters"]},{"cell_type":"code","execution_count":null,"id":"f7982611","metadata":{"id":"f7982611"},"outputs":[],"source":["import matplotlib\n","matplotlib.use('TkAgg')  # Use TkAgg backend for matplotlib (needed on some systems)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import csv\n","import itertools\n","from sklearn.metrics import precision_recall_fscore_support, mean_squared_error, mean_absolute_error, r2_score\n","\n","# ===========================\n","# Function: plot_confusion_matrix\n","# Visualizes a confusion matrix\n","# ===========================\n","def plot_confusion_matrix(cm, title='', cmap=plt.get_cmap(\"Blues\"), labels=[], save_filename=None):\n","    plt.figure(figsize=(10, 7))\n","    plt.imshow(cm*100, interpolation='nearest', cmap=cmap)  # Multiply by 100 for percentage\n","    plt.title(title, fontsize=10)\n","    plt.colorbar()\n","\n","    tick_marks = np.arange(len(labels))\n","    plt.xticks(tick_marks, labels, rotation=90, size=12)  # X-axis labels\n","    plt.yticks(tick_marks, labels, size=12)               # Y-axis labels\n","\n","    # Annotate each cell with the value\n","    for i in range(len(tick_marks)):\n","        for j in range(len(tick_marks)):\n","            if i != j:\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10)\n","            else:  # Diagonal (correct predictions)\n","                color = 'darkorange'  # Highlight diagonal cells\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10, color=color)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label', fontdict={'size':16,})\n","    plt.xlabel('Predicted label', fontdict={'size':16,})\n","\n","    # Save figure if filename is provided\n","    if save_filename is not None:\n","        plt.savefig(save_filename, format='pdf', dpi=1200, bbox_inches='tight')\n","    plt.close()\n","\n","# ===========================\n","# Function: calculate_confusion_matrix\n","# Computes confusion matrix and counts correct/incorrect predictions\n","# ===========================\n","def calculate_confusion_matrix(Y, Y_hat, classes):\n","    n_classes = len(classes)\n","    conf = np.zeros([n_classes, n_classes])      # Raw counts of predictions\n","    confnorm = np.zeros([n_classes, n_classes])  # Normalized per-class confusion\n","\n","    # Fill confusion matrix\n","    for k in range(Y.shape[0]):\n","        i = list(Y[k,:]).index(1)           # True label index (one-hot to integer)\n","        j = int(np.argmax(Y_hat[k,:]))      # Predicted label index\n","        conf[i,j] += 1\n","\n","    # Normalize confusion matrix by row\n","    for i in range(n_classes):\n","        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n","\n","    right = np.sum(np.diag(conf))  # Total correct predictions\n","    wrong = np.sum(conf) - right   # Total incorrect predictions\n","    return confnorm, right, wrong\n","\n","# ===========================\n","# Function: calculate_acc_at1snr_from_cm\n","# Computes per-class accuracy from normalized confusion matrix\n","# ===========================\n","def calculate_acc_at1snr_from_cm(cm):\n","    return np.round(np.diag(cm) / np.sum(cm, axis=1), 3)  # Diagonal / row sum\n","\n","# ===========================\n","# Function: calculate_metrics\n","# Computes overall metrics (accuracy, precision, recall, F1, etc.)\n","# ===========================\n","def calculate_metrics(Y, Y_hat):\n","    Y_true = np.argmax(Y, axis=1)  # Convert one-hot labels to integers\n","    Y_pred = np.argmax(Y_hat, axis=1)\n","\n","    # Compute metrics\n","    accuracy = accuracy_score(Y_true, Y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='weighted')\n","    mse = mean_squared_error(Y_true, Y_pred)\n","    mae = mean_absolute_error(Y_true, Y_pred)\n","    r2 = r2_score(Y_true, Y_pred)\n","\n","    return accuracy, precision, recall, f1\n","\n","# ===========================\n","# Function: calculate_acc_cm_each_snr\n","# Calculate and plot metrics at each SNR level\n","# ===========================\n","def calculate_acc_cm_each_snr(Y, Y_hat, Z, classes=None, save_figure=True, min_snr=0):\n","    Z_array = Z[:, 0]                        # Extract SNR values from Z\n","    snrs = sorted(list(set(Z_array)))        # Unique SNR levels\n","    acc = np.zeros(len(snrs))                # Accuracy for each SNR\n","    acc_mod_snr = np.zeros((len(classes), len(snrs)))  # Per-class accuracy per SNR\n","\n","    # Store metrics for plotting\n","    metrics = {\n","        'accuracy': [],\n","        'precision': [],\n","        'recall': [],\n","        'f1': []\n","    }\n","\n","    i = 0\n","    for snr in snrs:\n","        # Select samples corresponding to current SNR\n","        Y_snr = Y[np.where(Z_array == snr)]\n","        Y_hat_snr = Y_hat[np.where(Z_array == snr)]\n","\n","        # Calculate metrics\n","        accuracy, precision, recall, f1 = calculate_metrics(Y_snr, Y_hat_snr)\n","\n","        # Append metrics to dictionary\n","        metrics['accuracy'].append(accuracy)\n","        metrics['precision'].append(precision)\n","        metrics['recall'].append(recall)\n","        metrics['f1'].append(f1)\n","\n","    # --------------------------\n","    # Plot metrics vs SNR\n","    # --------------------------\n","    for metric, values in metrics.items():\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(snrs, values, label=metric)\n","        # Annotate values on plot\n","        for x, y in zip(snrs, values):\n","            plt.text(x, y, round(y, 3), ha='center', va='bottom', fontsize=8)\n","        plt.xlabel(\"Signal to Noise Ratio (dB)\")\n","        plt.ylabel(metric.capitalize())\n","        plt.title(f\"{metric.capitalize()} vs SNR\")\n","        plt.legend()\n","        plt.grid()\n","        # Save figure as PDF\n","        plt.savefig(f'figure/{metric}_vs_snr.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"36efb248","metadata":{"id":"36efb248"},"outputs":[],"source":["import os, random\n","\n","# ===========================\n","# Configure Keras and GPU settings\n","# ===========================\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"       # Set Keras backend to TensorFlow\n","# os.environ[\"THEANO_FLAGS\"] = \"device=gpu%d\"%(0) # Optional: If using Theano backend, select GPU 0\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"        # Specify which GPU to use (GPU 1)\n","\n","import numpy as np\n","import matplotlib\n","# matplotlib.use('Tkagg')  # Optional: Use TkAgg backend for matplotlib if needed\n","import matplotlib.pyplot as plt\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","# from matplotlib import pyplot as plt  # Alternative import (commented out)\n","import pickle, sys, h5py\n","import keras\n","import keras.backend as K\n","from keras.callbacks import LearningRateScheduler\n","from keras.regularizers import *\n","from keras.optimizers import Adam\n","from keras.models import model_from_json\n","# from keras.utils.vis_utils import plot_model  # Optional: visualize model structure\n","\n","# ===========================\n","# TensorFlow and Pandas\n","# ===========================\n","import tensorflow as tf\n","import pandas as pd\n","\n","# ===========================\n","# Keras utility for converting labels to one-hot encoding\n","# ===========================\n","from keras.utils.np_utils import to_categorical\n","\n","# ===========================\n","# Define the modulation classes for classification\n","# ===========================\n","classes = [\n","    'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '64PSK',\n","    '4QAM', '8QAM', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM',\n","    '2FSK', '4FSK', '8FSK', '16FSK',\n","    '4PAM', '8PAM', '16PAM',\n","    'AM-DSB', 'AM-DSB-SC', 'AM-USB', 'AM-LSB',\n","    'FM', 'PM'\n","]\n","# This list represents all the modulation schemes that the model will classify.\n","# It is used for one-hot encoding of labels, confusion matrix labeling, and plotting results."]},{"cell_type":"code","execution_count":null,"id":"b8b47bb9","metadata":{"id":"b8b47bb9"},"outputs":[],"source":["# ===========================\n","# Load training data\n","# ===========================\n","data1 = h5py.File('Dataset/HisarMod2019.1/Train/train.mat','r')  # Open .mat file in read mode\n","train = data1['data_save'][:]                                   # Extract dataset from HDF5 group 'data_save'\n","\n","# Swap axes to match model input shape\n","# Original shape might be (num_samples, 2, 1024) or (1024, 2, num_samples)\n","# After swapaxes(0,2), shape becomes (num_samples, 2, 1024)\n","train = train.swapaxes(0,2)\n","\n","# ===========================\n","# Load test data\n","# ===========================\n","data2 = h5py.File('Dataset/HisarMod2019.1/Test/test.mat','r')\n","test = data2['data_save'][:]\n","test = test.swapaxes(0,2)\n","\n","# ===========================\n","# Add channel dimension\n","# ===========================\n","# Conv2D layers expect input of shape (samples, height, width, channels)\n","train = np.expand_dims(train, axis=3)  # Add a channel dimension: shape -> (num_samples, 2, 1024, 1)\n","test = np.expand_dims(test, axis=3)"]},{"cell_type":"code","execution_count":null,"id":"24e87151","metadata":{"id":"24e87151"},"outputs":[],"source":["# ===========================\n","# Load and preprocess labels\n","# ===========================\n","\n","# Load training labels from CSV file\n","train_labels = pd.read_csv('Dataset/HisarMod2019.1/Train/train_labels1.csv', header=None)\n","train_labels = np.array(train_labels)  # Convert DataFrame to NumPy array\n","\n","# Convert integer labels to one-hot encoding\n","# This is required for categorical cross-entropy loss in Keras\n","train_labels = to_categorical(train_labels, num_classes=None)\n","\n","\n","# Load test labels from CSV file\n","test_labels = pd.read_csv('Dataset/HisarMod2019.1/Test/test_labels1.csv', header=None)\n","test_labels = np.array(test_labels)    # Convert DataFrame to NumPy array\n","test_labels = to_categorical(test_labels, num_classes=None)  # One-hot encoding\n","\n","\n","# ===========================\n","# Load and preprocess SNR values\n","# ===========================\n","\n","# Load training SNR values from CSV\n","# Each row corresponds to the SNR of a training sample\n","train_snr = pd.read_csv('Dataset/HisarMod2019.1/Train/train_snr.csv', header=None)\n","train_snr = np.array(train_snr)  # Convert to NumPy array for easier indexing\n","\n","# Load test SNR values from CSV\n","# Each row corresponds to the SNR of a test sample\n","test_snr = pd.read_csv('Dataset/HisarMod2019.1/Test/test_snr.csv', header=None)\n","test_snr = np.array(test_snr)    # Convert to NumPy array"]},{"cell_type":"code","execution_count":null,"id":"cc8c0680","metadata":{"id":"cc8c0680"},"outputs":[],"source":["# ===========================\n","# Split dataset into training and validation sets\n","# ===========================\n","\n","# Number of total examples in the training dataset\n","n_examples = train.shape[0]  # train has shape [N, 1024, 2]\n","\n","# Number of samples for training and validation\n","n_train = int(n_examples * 0.8)  # 80% for training\n","n_val = int(n_examples * 0.2)    # 20% for validation\n","\n","# Randomly select indices for training samples\n","train_idx = list(np.random.choice(range(0, n_examples), size=n_train, replace=False))\n","\n","# Remaining indices will be used for validation\n","val_idx = list(set(range(0, n_examples)) - set(train_idx))\n","\n","# Shuffle indices to ensure random ordering\n","np.random.shuffle(train_idx)\n","np.random.shuffle(val_idx)\n","\n","# ===========================\n","# Create datasets using the indices\n","# ===========================\n","\n","# Training data and labels\n","X_train = train[train_idx]         # Shape: [n_train, 1024, 2]\n","Y_train = train_labels[train_idx]  # Shape: [n_train, num_classes]\n","\n","# Validation data and labels\n","X_val = train[val_idx]             # Shape: [n_val, 1024, 2]\n","Y_val = train_labels[val_idx]      # Shape: [n_val, num_classes]\n","\n","# Test data and labels (use full test set)\n","X_test = test                     # Shape: [num_test_samples, 1024, 2]\n","Y_test = test_labels               # Shape: [num_test_samples, num_classes]\n","\n","# Test SNR values corresponding to each test sample\n","Z_test = test_snr                  # Shape: [num_test_samples, 1] or similar"]},{"cell_type":"code","execution_count":null,"id":"9ae0690e","metadata":{"id":"9ae0690e"},"outputs":[],"source":["# ===========================\n","# Set up training parameters\n","# ===========================\n","\n","nb_epoch = 200    # Total number of times the model will iterate over the entire training dataset\n","batch_size = 300  # Number of samples per gradient update during training\n","\n","\n","# ===========================\n","# Create and compile the model\n","# ===========================\n","\n","model = ResNet()  # Instantiate the ResNet model defined earlier\n","\n","# Compile the model with:\n","# - Loss: categorical_crossentropy (for multi-class classification)\n","# - Metrics: accuracy (to monitor performance during training)\n","# - Optimizer: Adam (adaptive learning rate optimizer)\n","model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adam')\n","\n","\n","# ===========================\n","# Visualize the model architecture\n","# ===========================\n","\n","# Save a graphical representation of the model architecture as 'model_Resnet.png'\n","# show_shapes=True ensures that the output shapes of each layer are displayed\n","plot_model(model, to_file='model_Resnet.png', show_shapes=True)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"3bd39df5","metadata":{"id":"3bd39df5"},"outputs":[],"source":["# ===========================\n","# Set file path to save the best model weights\n","# ===========================\n","filepath = 'weights/weights.h5'  # The model weights will be saved here during training\n","\n","\n","# ===========================\n","# Record the start time for training\n","# ===========================\n","import time\n","TRS_ResNet = time.time()  # Timestamp before training begins (used to calculate training duration)\n","\n","\n","# ===========================\n","# Train the ResNet model\n","# ===========================\n","history = model.fit(\n","    X_train,                   # Training data\n","    Y_train,                   # Training labels (one-hot encoded)\n","    batch_size=batch_size,     # Number of samples per gradient update\n","    epochs=nb_epoch,           # Total number of passes over the training data\n","    verbose=2,                 # Verbosity level: 2 = one line per epoch\n","    validation_data=(X_val, Y_val),  # Data for evaluating validation loss/accuracy after each epoch\n","    callbacks=[                # List of callback functions to customize training\n","        # Save the model weights whenever validation loss improves\n","        keras.callbacks.ModelCheckpoint(\n","            filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'\n","        ),\n","        # Reduce learning rate by factor 0.5 if validation loss plateaus for 5 epochs\n","        keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_loss', factor=0.5, verbose=1, patince=5, min_lr=0.0000001\n","        ),\n","        # Stop training early if validation loss does not improve for 5 epochs\n","        keras.callbacks.EarlyStopping(\n","            monitor='val_loss', patience=5, verbose=1, mode='auto'\n","        ),\n","        # Optional: TensorBoard callback for visualizing training (commented out)\n","        # keras.callbacks.TensorBoard(histogram_freq=1, write_graph=True, write_images=True)\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"fdddbf26","metadata":{"id":"fdddbf26"},"outputs":[],"source":["# ===========================\n","# Record end time of training\n","# ===========================\n","TRE_ResNet = time.time()  # Timestamp after training completes\n","\n","# ===========================\n","# Calculate total training time\n","# ===========================\n","T_ResNet = TRE_ResNet - TRS_ResNet  # Total training duration in seconds"]},{"cell_type":"code","execution_count":null,"id":"3f407071","metadata":{"id":"3f407071"},"outputs":[],"source":["# ===========================\n","# Evaluate model performance on test set\n","# ===========================\n","\n","TES_ResNet = time.time()  # Timestamp before evaluation (can be used to measure inference time)\n","\n","# Evaluate the trained model on the test data\n","# - Returns [loss, accuracy] because we compiled the model with loss='categorical_crossentropy' and metrics=['accuracy']\n","# - verbose=1 prints progress\n","# - batch_size=batch_size controls how many samples are processed at once during evaluation\n","score = model.evaluate(X_test, Y_test, verbose=1, batch_size=batch_size)\n","\n","# Print the evaluation results\n","# score[0] = test loss, score[1] = test accuracy\n","print(score)"]},{"cell_type":"code","execution_count":null,"id":"8b41ef40","metadata":{"id":"8b41ef40"},"outputs":[],"source":["calculate_acc_cm_each_snr(Y_test, test_Y_hat, Z_test, classes, min_snr=-18) #plot accuracy"]},{"cell_type":"code","execution_count":null,"id":"94ff4399","metadata":{"id":"94ff4399"},"outputs":[],"source":["# ===========================\n","# Record end time of test evaluation\n","# ===========================\n","TEE_ResNet = time.time()  # Timestamp after model evaluation on test set\n","\n","# ===========================\n","# Calculate total evaluation/inference time\n","# ===========================\n","T_ResNet = TEE_ResNet - TES_ResNet  # Total time taken to evaluate the model on the test data"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}