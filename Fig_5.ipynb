{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAO21r3E4CEy4QyQtMtf2n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5EHqn6wQRg2o"},"outputs":[],"source":["import os\n","from keras.models import Model\n","from keras.layers import Input, Dense, ReLU, Dropout, Softmax, Conv2D, MaxPool2D, Lambda, GaussianNoise\n","from keras.layers import Bidirectional, Flatten, CuDNNGRU\n","from keras.utils.vis_utils import plot_model\n","\n","# Define ICAMC model (custom CNN architecture)\n","def ICAMC(weights=None,\n","          input_shape=[2,1024],\n","          classes=26,\n","          **kwargs):\n","\n","    # Check if weights are valid\n","    if weights is not None and not (os.path.exists(weights)):\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization), '\n","                         'or the path to the weights file to be loaded.')\n","\n","    dr = 0.45   # Dropout rate for regularization\n","\n","    # Input layer\n","    input = Input(input_shape + [1], name='input')\n","\n","    # First convolution layer + pooling\n","    x = Conv2D(32, (1, 8), activation=\"relu\", name=\"conv1\",\n","               padding='same', kernel_initializer='glorot_uniform')(input)\n","    x = MaxPool2D(pool_size=(2, 2))(x)\n","\n","    # Second convolution layer\n","    x = Conv2D(32, (1, 4), activation=\"relu\", name=\"conv2\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","\n","    # Third convolution layer\n","    x = Conv2D(64, (1, 8), activation=\"relu\", name=\"conv3\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","    x = Dropout(dr)(x)   # Dropout for regularization\n","\n","    # Fourth convolution layer\n","    x = Conv2D(64, (1, 8), activation=\"relu\", name=\"conv4\",\n","               padding='same', kernel_initializer='glorot_uniform')(x)\n","    x = Dropout(dr)(x)   # Dropout again\n","\n","    # Flatten convolution outputs for dense layers\n","    x = Flatten()(x)\n","\n","    # Fully connected dense layer\n","    x = Dense(64, activation='relu', name='dense1')(x)\n","    x = Dropout(dr)(x)   # Dropout\n","\n","    # Add Gaussian noise for robustness against overfitting\n","    x = GaussianNoise(1)(x)\n","\n","    # Final classification layer (softmax for multiclass output)\n","    x = Dense(classes, activation='softmax', name='dense2')(x)\n","\n","    # Build model\n","    model = Model(inputs=input, outputs=x)\n","\n","    # Load pre-trained weights if provided\n","    if weights is not None:\n","        model.load_weights(weights)\n","\n","    return model\n","\n","\n","# Run only when executed directly\n","import keras\n","if __name__ == '__main__':\n","    # Create the model\n","    model = ICAMC(None, input_shape=[2,1024], classes=26)\n","\n","    # Compile with Adam optimizer & categorical crossentropy loss\n","    adam = keras.optimizers.Adam(\n","        learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n","        epsilon=None, decay=0.0, amsgrad=False\n","    )\n","    model.compile(loss='categorical_crossentropy',\n","                  metrics=['accuracy'],\n","                  optimizer=adam)\n","\n","    # Print model details\n","    print('Model layers:', model.layers)        # list of layers\n","    print('Model config:', model.get_config())  # config dictionary\n","    print('Model summary:')\n","    print(model.summary())                      # detailed summary"]},{"cell_type":"code","source":["import matplotlib\n","matplotlib.use('TkAgg')   # Use TkAgg backend for matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import csv\n","import itertools\n","from sklearn.metrics import precision_recall_fscore_support, mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n","\n","\n","# =======================\n","# Plot confusion matrix\n","# =======================\n","def plot_confusion_matrix(cm, title='', cmap=plt.get_cmap(\"Blues\"), labels=[], save_filename=None):\n","    plt.figure(figsize=(10, 7))\n","    plt.imshow(cm*100, interpolation='nearest', cmap=cmap)  # Multiply by 100 for percentage scale\n","    plt.title(title, fontsize=10)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(labels))\n","    # Add class labels to x and y axis\n","    plt.xticks(tick_marks, labels, rotation=90, size=12)\n","    plt.yticks(tick_marks, labels, size=12)\n","\n","    # Add values to each cell\n","    for i in range(len(tick_marks)):\n","        for j in range(len(tick_marks)):\n","            if i != j:\n","                # Off-diagonal entries: prediction errors\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10)\n","            else:\n","                # Diagonal entries: correct predictions (highlighted in orange)\n","                color = 'darkorange'\n","                plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10, color=color)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label', fontdict={'size':16,})\n","    plt.xlabel('Predicted label', fontdict={'size':16,})\n","\n","    # Save confusion matrix plot if filename provided\n","    if save_filename is not None:\n","        plt.savefig(save_filename, format='pdf', dpi=1200, bbox_inches='tight')\n","    plt.close()\n","\n","# =======================\n","# Calculate confusion matrix\n","# =======================\n","def calculate_confusion_matrix(Y, Y_hat, classes):\n","    n_classes = len(classes)\n","    conf = np.zeros([n_classes, n_classes])      # Raw confusion matrix\n","    confnorm = np.zeros([n_classes, n_classes])  # Normalized confusion matrix\n","\n","    # Fill confusion matrix by comparing true vs predicted labels\n","    for k in range(0, Y.shape[0]):\n","        i = list(Y[k,:]).index(1)           # True class index\n","        j = int(np.argmax(Y_hat[k,:]))      # Predicted class index\n","        conf[i,j] = conf[i,j] + 1\n","\n","    # Normalize rows (per-class accuracy)\n","    for i in range(0, n_classes):\n","        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n","\n","    # Count correct and incorrect predictions\n","    right = np.sum(np.diag(conf))\n","    wrong = np.sum(conf) - right\n","    return confnorm, right, wrong\n","\n","# =======================\n","# Calculate per-class accuracy at given SNR\n","# =======================\n","def calculate_acc_at1snr_from_cm(cm):\n","    return np.round(np.diag(cm) / np.sum(cm, axis=1), 3)\n","\n","# =======================\n","# Calculate metrics: Accuracy\n","# =======================\n","def calculate_metrics(Y, Y_hat):\n","    Y_true = np.argmax(Y, axis=1)       # Convert one-hot to label index\n","    Y_pred = np.argmax(Y_hat, axis=1)\n","    accuracy = accuracy_score(Y_true, Y_pred)\n","\n","    return accuracy\n","\n","# =======================\n","# Calculate accuracy & metrics per SNR\n","# =======================\n","def calculate_acc_cm_each_snr(Y, Y_hat, Z, classes=None, save_figure=True, min_snr=0):\n","    Z_array = Z[:, 0]                # Extract SNR values\n","    snrs = sorted(list(set(Z_array)))  # Unique SNRs\n","    acc = np.zeros(len(snrs))          # Store overall accuracy per SNR\n","    acc_mod_snr = np.zeros((len(classes), len(snrs)))  # Store per-class accuracy\n","\n","    # Dictionary to store metrics at each SNR\n","    metrics = {\n","        'accuracy': []\n","    }\n","\n","    i = 0\n","    for snr in snrs:\n","        # Select data corresponding to this SNR\n","        Y_snr = Y[np.where(Z_array == snr)]\n","        Y_hat_snr = Y_hat[np.where(Z_array == snr)]\n","\n","        # Compute confusion matrix\n","        cm, right, wrong = calculate_confusion_matrix(Y_snr, Y_hat_snr, classes)\n","        accuracy = calculate_metrics(Y_snr, Y_hat_snr)\n","\n","        # Store metrics\n","        metrics['accuracy'].append(accuracy)\n","\n","\n","        # Plot confusion matrix if above threshold SNR\n","        if snr >= min_snr:\n","            plot_confusion_matrix(cm, cmap=plt.cm.Blues, labels=classes, save_filename='figure/cm_snr{}.pdf'.format(snr))\n","\n","        # Overall accuracy for this SNR\n","        acc[i] = round(1.0 * right / (right + wrong), 3)\n","        print('Accuracy at %ddb: %.2f%s / (%d + %d)' % (snr, 100*acc[i], '%', right, wrong))\n","        acc_mod_snr[:, i] = calculate_acc_at1snr_from_cm(cm)\n","        i += 1\n","\n","    # Save accuracy results into a file\n","    fd = open('acc_overall_128k_on_512k_wts.dat', 'wb')\n","    pickle.dump(('128k', '512k', acc), fd)\n","    fd.close()\n","\n","    # Plot Accuracy\n","    for metric, values in metrics.items():\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(snrs, values, label=metric)\n","        for x, y in zip(snrs, values):\n","            plt.text(x, y, round(y, 3), ha='center', va='bottom', fontsize=8)\n","        plt.xlabel(\"Signal to Noise Ratio\")\n","        plt.ylabel(metric.capitalize())\n","        plt.title(f\"{metric.capitalize()} vs SNR\")\n","        plt.legend()\n","        plt.grid()\n","        plt.savefig(f'figure/{metric}_vs_snr.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n","        plt.show()"],"metadata":{"id":"Q6PH4BGjR7MI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","\n","# Set Keras backend to TensorFlow\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Select which GPU to use (here GPU 0 is visible, others hidden)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","# Imports for plotting colored line segments\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","\n","import sys, h5py\n","import tensorflow as tf\n","import pandas as pd\n","\n","# Utility function from Keras to convert labels to one-hot encoding\n","from keras.utils.np_utils import to_categorical\n","\n","# List of modulation classes used in the dataset (26 total)\n","classes = [\n","    'BPSK',        # Binary Phase Shift Keying\n","    'QPSK',        # Quadrature Phase Shift Keying\n","    '8PSK',        # 8-level Phase Shift Keying\n","    '16PSK',       # 16-level Phase Shift Keying\n","    '32PSK',       # 32-level Phase Shift Keying\n","    '64PSK',       # 64-level Phase Shift Keying\n","    '4QAM',        # 4-level Quadrature Amplitude Modulation\n","    '8QAM',        # 8-level Quadrature Amplitude Modulation\n","    '16QAM',       # 16-level Quadrature Amplitude Modulation\n","    '32QAM',       # 32-level Quadrature Amplitude Modulation\n","    '64QAM',       # 64-level Quadrature Amplitude Modulation\n","    '128QAM',      # 128-level Quadrature Amplitude Modulation\n","    '256QAM',      # 256-level Quadrature Amplitude Modulation\n","    '2FSK',        # 2-level Frequency Shift Keying\n","    '4FSK',        # 4-level Frequency Shift Keying\n","    '8FSK',        # 8-level Frequency Shift Keying\n","    '16FSK',       # 16-level Frequency Shift Keying\n","    '4PAM',        # 4-level Pulse Amplitude Modulation\n","    '8PAM',        # 8-level Pulse Amplitude Modulation\n","    '16PAM',       # 16-level Pulse Amplitude Modulation\n","    'AM-DSB',      # Amplitude Modulation - Double Sideband\n","    'AM-DSB-SC',   # Amplitude Modulation - Double Sideband Suppressed Carrier\n","    'AM-USB',      # Amplitude Modulation - Upper Sideband\n","    'AM-LSB',      # Amplitude Modulation - Lower Sideband\n","    'FM',          # Frequency Modulation\n","    'PM'           # Phase Modulation\n","]"],"metadata":{"id":"BBFoDfqDSeRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load training and testing data\n","\n","#These are changed based on Dataset (RML2016.10a, RML2016.10b are RML2018.01a)\n","\n","#/HisarMod2019.1\n","# Open training dataset file (.mat format) with h5py\n","data1 = h5py.File('Dataset/HisarMod2019.1/Train/train.mat', 'r')\n","# Extract data array stored under key 'data_save'\n","train = data1['data_save'][:]\n","# Rearrange dimensions: move axis 0 to the end (needed for Keras/TensorFlow input format)\n","train = train.swapaxes(0, 2)\n","\n","# Open testing dataset file (.mat format)\n","data2 = h5py.File('Dataset/HisarMod2019.1/Test/test.mat', 'r')\n","# Extract test data\n","test = data2['data_save'][:]\n","# Rearrange dimensions for consistency\n","test = test.swapaxes(0, 2)\n","\n","# Add a channel dimension at the end (axis=3)\n","# This converts data into shape (samples, height, width, channels),\n","# which is the standard input format for Conv2D in Keras\n","train = np.expand_dims(train, axis=3)\n","test = np.expand_dims(test, axis=3)"],"metadata":{"id":"ybOeAdmhSkcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load and preprocess labels for training and testing data\n","\n","# -------------------------\n","# Training labels\n","# -------------------------\n","# Read CSV file containing training labels (no header in CSV)\n","train_labels = pd.read_csv('Dataset/HisarMod2019.1/Train/train_labels1.csv', header=None)\n","# Convert pandas DataFrame to numpy array\n","train_labels = np.array(train_labels)\n","# Convert labels to one-hot encoding for classification\n","# Example: label '3' becomes [0,0,0,1,0,...]\n","train_labels = to_categorical(train_labels, num_classes=None)\n","\n","# -------------------------\n","# Testing labels\n","# -------------------------\n","# Read CSV file containing testing labels\n","test_labels = pd.read_csv('Dataset/HisarMod2019.1/Test/test_labels1.csv', header=None)\n","# Convert to numpy array\n","test_labels = np.array(test_labels)\n","# Convert testing labels to one-hot encoding\n","test_labels = to_categorical(test_labels, num_classes=None)"],"metadata":{"id":"qW3nMIR3SwYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load Signal-to-Noise Ratio (SNR) values for training and testing data\n","\n","# -------------------------\n","# Training SNR\n","# -------------------------\n","# Read CSV file containing SNR values for each training sample (no header in CSV)\n","train_snr = pd.read_csv('Dataset/HisarMod2019.1/Train/train_snr.csv', header=None)\n","# Convert pandas DataFrame to numpy array for easier manipulation\n","train_snr = np.array(train_snr)\n","\n","# -------------------------\n","# Testing SNR\n","# -------------------------\n","# Read CSV file containing SNR values for each testing sample\n","test_snr = pd.read_csv('Dataset/HisarMod2019.1/Test/test_snr.csv', header=None)\n","# Convert to numpy array\n","test_snr = np.array(test_snr)"],"metadata":{"id":"0AwPhnZMTJgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","from numpy import linalg as la\n","# RML2016.10a dataset loader and preprocessing\n","\n","maxlen = 128   # maximum sequence length (IQ samples)\n","\n","# L2 normalization along a given axis\n","def l2_normalize(x, axis=-1):\n","    # compute squared norm, take max along axis, then normalize\n","    y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True)\n","    return x / np.sqrt(y)\n","\n","# Normalize each sample (first channel) to unit L2-norm\n","def norm_pad_zeros(X_train, nsamples):\n","    print(\"Pad:\", X_train.shape)\n","    for i in range(X_train.shape[0]):\n","        # normalize real part (channel 0) by its L2 norm\n","        X_train[i, :, 0] = X_train[i, :, 0] / la.norm(X_train[i, :, 0], 2)\n","    return X_train\n","\n","\n","# Convert I/Q (real, imag) to amplitude and phase representation\n","def to_amp_phase(X_train, X_val, X_test, nsamples):\n","    # complex representation: I + jQ\n","    X_train_cmplx = X_train[:, 0, :] + 1j * X_train[:, 1, :]\n","    X_val_cmplx = X_val[:, 0, :] + 1j * X_val[:, 1, :]\n","    X_test_cmplx = X_test[:, 0, :] + 1j * X_test[:, 1, :]\n","\n","    # amplitude and normalized angle for training set\n","    X_train_amp = np.abs(X_train_cmplx)\n","    X_train_ang = np.arctan2(X_train[:, 1, :], X_train[:, 0, :]) / np.pi\n","    X_train_amp = np.reshape(X_train_amp, (-1, 1, nsamples))\n","    X_train_ang = np.reshape(X_train_ang, (-1, 1, nsamples))\n","    X_train = np.concatenate((X_train_amp, X_train_ang), axis=1)\n","    X_train = np.transpose(np.array(X_train), (0, 2, 1))\n","\n","    # same processing for validation set\n","    X_val_amp = np.abs(X_val_cmplx)\n","    X_val_ang = np.arctan2(X_val[:, 1, :], X_val[:, 0, :]) / np.pi\n","    X_val_amp = np.reshape(X_val_amp, (-1, 1, nsamples))\n","    X_val_ang = np.reshape(X_val_ang, (-1, 1, nsamples))\n","    X_val = np.concatenate((X_val_amp, X_val_ang), axis=1)\n","    X_val = np.transpose(np.array(X_val), (0, 2, 1))\n","\n","    # same processing for test set\n","    X_test_amp = np.abs(X_test_cmplx)\n","    X_test_ang = np.arctan2(X_test[:, 1, :], X_test[:, 0, :]) / np.pi\n","    X_test_amp = np.reshape(X_test_amp, (-1, 1, nsamples))\n","    X_test_ang = np.reshape(X_test_ang, (-1, 1, nsamples))\n","    X_test = np.concatenate((X_test_amp, X_test_ang), axis=1)\n","    X_test = np.transpose(np.array(X_test), (0, 2, 1))\n","\n","    return (X_train, X_val, X_test)\n","\n","\n","# Load RML2016.10a dataset and prepare train/val/test splits\n","def load_data(filename=r'E:\\RML_2016_10A\\data\\RML2016.10a_dict.pkl'):\n","    # load pickle file: dictionary with keys = (modulation, SNR)\n","    Xd = pickle.load(open(filename, 'rb'), encoding='iso-8859-1')\n","    # extract list of modulation types and SNR values\n","    mods, snrs = [sorted(list(set([k[j] for k in Xd.keys()]))) for j in [0, 1]]\n","    # mods: e.g. ['8PSK','AM-DSB','BPSK',...]\n","\n","    X = []         # store signal data\n","    lbl = []       # store labels (mod, snr)\n","    train_idx = [] # indices for training\n","    val_idx = []   # indices for validation\n","    np.random.seed(2016) # reproducibility\n","    a = 0\n","\n","    # collect data for each modulation × SNR pair\n","    for mod in mods:\n","        for snr in snrs:\n","            X.append(Xd[(mod, snr)])  # shape (6000, 2, 128)\n","            for i in range(Xd[(mod, snr)].shape[0]):\n","                lbl.append((mod, snr))\n","            # randomly split indices: 600 train, 200 val, rest test\n","            train_idx += list(np.random.choice(range(a*1000, (a+1)*1000), size=600, replace=False))\n","            val_idx += list(np.random.choice(list(set(range(a*1000, (a+1)*1000)) - set(train_idx)), size=200, replace=False))\n","            a += 1\n","\n","    # stack all signals into one big numpy array\n","    X = np.vstack(X)\n","    n_examples = X.shape[0]\n","\n","    # remaining indices go to test set\n","    test_idx = list(set(range(0, n_examples)) - set(train_idx) - set(val_idx))\n","\n","    # shuffle index lists\n","    np.random.shuffle(train_idx)\n","    np.random.shuffle(val_idx)\n","    np.random.shuffle(test_idx)\n","\n","    # split data\n","    X_train = X[train_idx]\n","    X_val   = X[val_idx]\n","    X_test  = X[test_idx]\n","\n","    # helper: convert labels to one-hot encoding\n","    def to_onehot(yy):\n","        yy1 = np.zeros([len(yy), len(mods)])\n","        yy1[np.arange(len(yy)), yy] = 1\n","        return yy1\n","\n","    # one-hot labels for train/val/test\n","    Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n","    Y_val   = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), val_idx)))\n","    Y_test  = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n","\n","    # convert to amplitude/phase representation\n","    X_train, X_val, X_test = to_amp_phase(X_train, X_val, X_test, 128)\n","\n","    # cut or pad to maxlen\n","    X_train = X_train[:, :maxlen, :]\n","    X_val   = X_val[:, :maxlen, :]\n","    X_test  = X_test[:, :maxlen, :]\n","\n","    # normalize real parts\n","    X_train = norm_pad_zeros(X_train, maxlen)\n","    X_val   = norm_pad_zeros(X_val, maxlen)\n","    X_test  = norm_pad_zeros(X_test, maxlen)\n","\n","    # shapes check\n","    print(X_train.shape)\n","    print(X_val.shape)\n","    print(X_test.shape)\n","    print(Y_train.shape)\n","    print(Y_val.shape)\n","    print(Y_test.shape)\n","\n","    return (mods, snrs, lbl), (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), (train_idx, val_idx, test_idx)\n","\n","\n","# Run script directly: load dataset and preprocess\n","if __name__ == '__main__':\n","    (mods, snrs, lbl), (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), (train_idx, val_idx, test_idx) = load_data()"],"metadata":{"id":"R9TmiIs8dRoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RML2016.10b dataset loader and preprocessing\n","\n","def load_data(filename=r'E:\\RML_2016_10B\\data\\RML2016.10b.dat'):\n","    # Load dataset from pickle file (dictionary: keys=(modulation, SNR), values=(samples))\n","    Xd = pickle.load(open(filename, 'rb'), encoding='iso-8859-1')\n","    # Extract list of modulation types and SNR values from dictionary keys\n","    mods, snrs = [sorted(list(set([k[j] for k in Xd.keys()]))) for j in [0, 1]]\n","\n","    X = []          # all samples (stacked later)\n","    lbl = []        # labels: (modulation, snr) for each sample\n","    train_idx = []  # training indices\n","    val_idx = []    # validation indices\n","    np.random.seed(2016)  # reproducibility\n","    a = 0           # counter to shift index ranges\n","\n","    # Iterate over all modulation × SNR combinations\n","    for mod in mods:\n","        for snr in snrs:\n","            # Append the samples for this (mod, snr) pair → shape (1000, 2, 128)\n","            X.append(Xd[(mod, snr)])\n","            # Create label list (same length as number of samples)\n","            for i in range(Xd[(mod, snr)].shape[0]):\n","                lbl.append((mod, snr))\n","            # Randomly select 3600 training indices from this block of 6000\n","            train_idx += list(np.random.choice(range(a*6000, (a+1)*6000), size=3600, replace=False))\n","            # Randomly select 1200 validation indices (disjoint from training set)\n","            val_idx += list(np.random.choice(list(set(range(a*6000, (a+1)*6000)) - set(train_idx)), size=1200, replace=False))\n","            a += 1   # move to next block\n","\n","    # Stack into one big array: shape (220000, 2, 128)\n","    X = np.vstack(X)\n","    print(len(lbl))    # total number of labels (should match X.shape[0])\n","\n","    n_examples = X.shape[0]\n","    # Remaining indices are assigned to the test set\n","    test_idx = list(set(range(0, n_examples)) - set(train_idx) - set(val_idx))\n","\n","    # Shuffle all index lists\n","    np.random.shuffle(train_idx)\n","    np.random.shuffle(val_idx)\n","    np.random.shuffle(test_idx)\n","\n","    # Split dataset into train, validation, and test sets\n","    X_train = X[train_idx]\n","    X_val   = X[val_idx]\n","    X_test  = X[test_idx]\n","\n","    # Print counts\n","    print(len(train_idx))\n","    print(len(val_idx))\n","    print(len(test_idx))\n","\n","    # Print dataset shapes\n","    print(X_train.shape)\n","    print(X_val.shape)\n","    print(X_test.shape)\n","\n","    # Helper function: convert labels into one-hot vectors\n","    def to_onehot(yy):\n","        yy1 = np.zeros([len(yy), len(mods)])  # (#samples, #modulation classes)\n","        yy1[np.arange(len(yy)), yy] = 1\n","        return yy1\n","\n","    # One-hot encode labels for train/val/test\n","    Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n","    Y_val   = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), val_idx)))\n","    Y_test  = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n","\n","    # Print shapes of labels\n","    print(Y_train.shape)\n","    print(Y_val.shape)\n","    print(Y_test.shape)\n","\n","    # Return everything\n","    return (mods, snrs, lbl), (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), (train_idx, val_idx, test_idx)\n","\n","\n","# Run script directly: load dataset and split\n","if __name__ == '__main__':\n","    (mods, snrs, lbl), (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), (train_idx, val_idx, test_idx) = load_data()"],"metadata":{"id":"bh2Bpqn6d9aW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RML2018.01a dataset loader and preprocessing\n","\n","def sliceX_normalized(from_filename='/media/GOLD_XYZ_OSC.0001_1024.hdf5',\n","                      slice_len=1024,\n","                      to_filename='/media/XYZ_1024_2m_norm.hdf5'):\n","    \"\"\"\n","    Normalize raw RML2018.01a dataset slices and save to new HDF5 file.\n","    Each sample is normalized with min-max scaling to range [-10, 10].\n","    \"\"\"\n","    from_file = h5py.File(from_filename, 'r')\n","    X = from_file['X']   # raw IQ samples\n","    Y = from_file['Y']   # one-hot encoded modulation labels\n","    Z = from_file['Z']   # SNR labels\n","\n","    # create new HDF5 file for normalized data\n","    to_file = h5py.File(to_filename, 'w')\n","    X_slice = to_file.create_dataset(\n","        name='X',\n","        shape=(X.shape[0], slice_len, X.shape[2]),\n","        maxshape=(None, slice_len, X.shape[2]),\n","        dtype=np.float32,\n","        compression='gzip',\n","        chunks=(1, slice_len, X.shape[2])\n","    )\n","    Y_slice = to_file.create_dataset(\n","        name='Y',\n","        shape=Y.shape,\n","        maxshape=(None, Y.shape[1]),\n","        dtype=np.uint8,\n","        compression='gzip',\n","        chunks=(1, Y.shape[1])\n","    )\n","    Z_slice = to_file.create_dataset(\n","        name='Z',\n","        shape=Z.shape,\n","        dtype=np.int,\n","        compression='gzip'\n","    )\n","\n","    # normalize batch by batch\n","    batch_size = 4096\n","    for i in range(int(X.shape[0] / batch_size)):\n","        # take a slice of data\n","        batch_X = X[i * batch_size:(i + 1) * batch_size, 0:slice_len, :]\n","\n","        # normalize each sample inside the batch\n","        for j in range(batch_X.shape[0]):\n","            x = batch_X[j]\n","            n_s0, n_s1 = x.shape[0], x.shape[1]\n","            x = np.reshape(x, (n_s0 * n_s1,))\n","            # min-max scaling to [-10, 10]\n","            x = sklearn.preprocessing.minmax_scale(x, feature_range=(-10, 10))\n","            x = np.reshape(x, (n_s0, n_s1))\n","            batch_X[j] = x\n","\n","        # save normalized data into new HDF5 file\n","        X_slice[i * batch_size:(i + 1) * batch_size, :, :] = batch_X\n","        Y_slice[i * batch_size:(i + 1) * batch_size, :] = Y[i * batch_size:(i + 1) * batch_size, :]\n","        Z_slice[i * batch_size:(i + 1) * batch_size] = Z[i * batch_size:(i + 1) * batch_size]\n","        print('write batch {}-{}'.format(i * batch_size, (i + 1) * batch_size))\n","\n","    from_file.close()\n","    to_file.close()\n","\n","\n","def subsample_data_2018_tofile(from_filename=\"/media/XYZ_1024_2m_norm.hdf5\",\n","                               sample_rate=1/12,\n","                               to_filename=\"/media/norm_XYZ_1024_512k.hdf5\"):\n","    \"\"\"\n","    Subsample the normalized dataset to reduce size.\n","    Keeps `sample_rate` fraction of samples per SNR group.\n","    \"\"\"\n","    f = h5py.File(from_filename, 'r')\n","    X = f['X']\n","    Y = f['Y']\n","    Z = f['Z']\n","\n","    to_file = h5py.File(to_filename, 'w')\n","    n_subsample = int(X.shape[0] * sample_rate)\n","\n","    # allocate new datasets for subsampled data\n","    X_slice = to_file.create_dataset(\n","        name='X',\n","        shape=(n_subsample, X.shape[1], X.shape[2]),\n","        maxshape=(None, X.shape[1], X.shape[2]),\n","        dtype=np.float32,\n","        compression='gzip',\n","        chunks=(1, X.shape[1], X.shape[2])\n","    )\n","    Y_slice = to_file.create_dataset(\n","        name='Y',\n","        shape=(n_subsample, Y.shape[1]),\n","        maxshape=(None, Y.shape[1]),\n","        dtype=np.uint8,\n","        compression='gzip',\n","        chunks=(1, Y.shape[1])\n","    )\n","    Z_slice = to_file.create_dataset(\n","        name='Z',\n","        shape=(n_subsample, 1),\n","        dtype=np.int,\n","        compression='gzip'\n","    )\n","\n","    snr_count = 26  # total number of SNR levels\n","    batch_size = int(X.shape[0] / snr_count)\n","    n_slice = int(batch_size * sample_rate)\n","\n","    # random sample from each SNR group\n","    for i in range(snr_count):\n","        print('subsample the snr {}'.format(i))\n","        batch_X = X[i * batch_size:(i + 1) * batch_size]\n","        batch_Y = Y[i * batch_size:(i + 1) * batch_size]\n","        batch_Z = Z[i * batch_size:(i + 1) * batch_size]\n","\n","        np.random.seed(2016)\n","        rand_idx = np.random.choice(np.arange(0, batch_size), size=n_slice, replace=False)\n","\n","        X_slice[i * n_slice:(i + 1) * n_slice, :, :] = batch_X[rand_idx]\n","        Y_slice[i * n_slice:(i + 1) * n_slice, :] = batch_Y[rand_idx]\n","        Z_slice[i * n_slice:(i + 1) * n_slice] = batch_Z[rand_idx]\n","\n","    to_file.close()\n","    f.close()\n","    print('subsample complete. total samples: {}'.format(n_subsample))\n","\n","\n","def load_data_2018(from_filename=\"/media/XYZ.0001_0512_NORM.hdf5\"):\n","    \"\"\"\n","    Load normalized RML2018 dataset from file.\n","    Returns X (IQ samples), Y (labels), Z (SNRs).\n","    \"\"\"\n","    f = h5py.File(from_filename, 'r')\n","    X = f['X'][:]\n","    Y = f['Y'][:]\n","    Z = f['Z'][:]\n","    f.close()\n","    return X, Y, Z\n","\n","\n","def data_analyse(filename=None):\n","    \"\"\"\n","    Analyse dataset min/max values for sanity check (before & after normalization).\n","    \"\"\"\n","    import pandas as pd\n","    X, _, _ = load_data_2018(from_filename=filename)\n","    pX = pd.Panel(X)\n","    pX_max = pX.max()\n","    print('max(I,Q) of the origin data is {}\\r'.format(pX_max.max(axis=1)))\n","    pX_min = pX.min()\n","    print('min(I) of the origin data is {}\\r'.format(pX_min.min(axis=1)))\n","    print('=========================================================')\n","\n","    Xn, _, _ = load_data_2018(from_filename=filename)\n","    pXn = pd.Panel(Xn)\n","    pXn_max = pXn.max()\n","    print('max(I,Q) of the normalized data is {}'.format(pXn_max.max(axis=1)))\n","    pXn_min = pXn.min()\n","    print('min(I) of the normalized data is {}'.format(pXn_min.min(axis=1)))\n","\n","\n","def data_structure(filename='/media/XYZ_1024_512k.hdf5'):\n","    \"\"\"\n","    Inspect HDF5 dataset structure, print shapes and distributions of X, Y, Z.\n","    Also visualize distributions of SNRs and modulation labels.\n","    \"\"\"\n","    f = h5py.File(filename, 'r')\n","    print('hdf5 keys include 3 DataSet:')\n","    for k in f.keys():\n","        print(k)\n","\n","    print('each dataset shape:')\n","    print('X:{}, Y:{}, Z:{}'.format(f['X'].shape, f['Y'].shape, f['Z'].shape))\n","\n","    print('each dataset type:')\n","    print(type(f['X']), type(f['Y']), type(f['Z']))\n","\n","    print('each dataset element shape:')\n","    print('X[i]:{}, Y[i]:{}, Z[i]:{}'.format(f['X'][0].shape, f['Y'][0].shape, f['Z'][0].shape))\n","\n","    print('Y element like:')\n","    print(f['Y'][0])\n","\n","    snrs = sorted(list(set(f['Z'][:, 0])))\n","    print('Z element include:')\n","    print(snrs)\n","\n","    # visualize SNR distribution\n","    Z = f['Z']\n","    lenZ = int(4096 * 24 / 32)\n","    plt.figure()\n","    plt.title('Z snr distribute')\n","    plt.plot(np.arange(lenZ), Z[0:lenZ])\n","    plt.show()\n","\n","    # visualize modulation distribution\n","    Y = f['Y']\n","    sumY = np.sum(Y, axis=0)\n","    print(sumY)\n","    Y_not_onehot = np.argmax(Y, axis=1)\n","    print(Y_not_onehot)\n","    lenY = Y_not_onehot.shape[0]\n","    plt.figure()\n","    plt.title('Y mod distribute')\n","    plt.plot(np.arange(lenY), Y_not_onehot[0:lenY])\n","    plt.show()\n","\n","    # inspect one sample signal\n","    oneX = f['X'][0]\n","    print(oneX)\n","    oneX = np.power(oneX, 2)\n","    print(oneX)\n","    oneX = np.sqrt(oneX[:, 0] + oneX[:, 1])\n","    print(oneX)\n","    print(np.sum(oneX))\n","\n","    f.close()\n","\n","\n","if __name__ == '__main__':\n","    # Example: subsample dataset to 1/12th size and save\n","    subsample_data_2018_tofile(from_filename=r\"\\GOLD_XYZ_OSC.0001_1024.hdf5\",\n","                               sample_rate=1/12,\n","                               to_filename=r\"\\XYZ_1024_1_12.hdf5\")\n","    pass"],"metadata":{"id":"hYrDXY2lfJ-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Split dataset into training, validation, and testing sets\n","# ===========================\n","\n","# Total number of examples in the training data\n","n_examples = train.shape[0]\n","\n","# Define number of training samples (60% of total)\n","n_train = int(n_examples * 0.6)\n","\n","# Define number of validation samples (15% of total)\n","n_val = int(n_examples * 0.15)\n","\n","# Randomly select indices for training samples without replacement\n","train_idx = list(np.random.choice(range(0, n_examples), size=n_train, replace=False))\n","\n","# Remaining indices are used for validation\n","val_idx = list(set(range(0, n_examples)) - set(train_idx))\n","\n","# Shuffle the indices to ensure randomness\n","np.random.shuffle(train_idx)\n","np.random.shuffle(val_idx)\n","\n","# -------------------------\n","# Prepare training and validation sets\n","# -------------------------\n","X_train = train[train_idx]           # Training data samples\n","Y_train = train_labels[train_idx]    # Corresponding one-hot training labels\n","\n","X_val = train[val_idx]               # Validation data samples\n","Y_val = train_labels[val_idx]        # Corresponding one-hot validation labels\n","\n","# -------------------------\n","# Testing set\n","# -------------------------\n","X_test = test                        # Test data samples\n","Y_test = test_labels                 # Test labels (one-hot encoded)\n","Z_test = test_snr                     # SNR values for each test sample"],"metadata":{"id":"smgOu26mTNkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up some params\n","nb_epoch = 200     # number of epochs to train on\n","batch_size = 300  # training batch size"],"metadata":{"id":"9yLT3r4DTW6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Initialize and compile the model\n","# ===========================\n","\n","# Create an instance of the ICAMC model\n","model = ICAMC()\n","\n","# Compile the model\n","# - Loss: 'categorical_crossentropy' (for multi-class classification)\n","# - Metrics: 'accuracy' to monitor performance\n","# - Optimizer: 'adam', a popular adaptive optimizer\n","model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","\n","# Plot the model architecture and save it as an image\n","# - 'show_shapes=True' displays the input/output shape for each layer\n","plot_model(model, to_file='model.png', show_shapes=True)  # Saves model diagram to 'model.png'\n","\n","# Print a detailed summary of the model to console\n","# - Shows layer types, output shapes, and number of parameters\n","model.summary()"],"metadata":{"id":"H5pAnhAqTa1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Define file path to save model weights\n","# ===========================\n","filepath = 'weights/weights.h5'  # Path where trained model weights will be saved\n","\n","# ===========================\n","# Record the start time for training\n","# ===========================\n","import time\n","TRS_PROPOSED = time.time()  # Store current time (seconds since epoch) to measure training duration later"],"metadata":{"id":"RPbpPiBdTedO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Train the model\n","# ===========================\n","\n","history = model.fit(\n","    X_train,                 # Training data inputs\n","    Y_train,                 # Training data labels (one-hot encoded)\n","    batch_size=batch_size,   # Number of samples per gradient update\n","    epochs=nb_epoch,         # Number of complete passes through the training dataset\n","    verbose=2,               # Verbosity mode (2 = one line per epoch)\n","    validation_data=(X_val, Y_val),  # Data for validation at the end of each epoch\n","    callbacks=[              # List of callbacks to apply during training\n","        # --------------------------\n","        # Save the best model weights based on validation loss\n","        # --------------------------\n","        keras.callbacks.ModelCheckpoint(\n","            filepath,                 # File path to save the model weights\n","            monitor='val_loss',        # Metric to monitor (validation loss)\n","            verbose=1,                 # Print messages when saving\n","            save_best_only=True,       # Only save the model if the monitored metric improves\n","            mode='auto'                # Let Keras decide whether to minimize or maximize the monitored metric\n","        ),\n","        # --------------------------\n","        # Reduce learning rate when validation loss plateaus\n","        # --------------------------\n","        keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_loss',       # Metric to monitor\n","            factor=0.5,               # Factor to reduce the learning rate by\n","            verbose=1,                # Print messages when reducing LR\n","            patince=5,                # Number of epochs with no improvement before reducing LR (note: should be 'patience', check typo)\n","            min_lr=0.0000001          # Minimum learning rate\n","        ),\n","        # --------------------------\n","        # Early stopping to prevent overfitting\n","        # --------------------------\n","        keras.callbacks.EarlyStopping(\n","            monitor='val_loss',       # Metric to monitor\n","            patience=5,               # Stop training if no improvement after 5 epochs\n","            verbose=1,                # Print message when stopping\n","            mode='auto'               # Let Keras decide whether to minimize or maximize the monitored metric\n","        )\n","    ]\n",")"],"metadata":{"id":"_83BlNanTihN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Record the end time and calculate total training duration\n","# ===========================\n","\n","TRE_PROPOSED = time.time()           # Capture the current time after training finishes\n","T_PROPOSED = TRE_PROPOSED - TRS_PROPOSED  # Calculate total training time (seconds) by subtracting start time\n","T_PROPOSED                           # Display or store the total training duration"],"metadata":{"id":"ms1S55M0TtUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Evaluate the trained model on the test set\n","# ===========================\n","\n","TES_PROPOSED = time.time()  # Record the current time before evaluation (optional, for timing)\n","\n","# Evaluate the model on test data\n","# - Returns a list: [loss, accuracy] (because we compiled the model with 'accuracy' metric)\n","score = model.evaluate(\n","    X_test,            # Test data inputs\n","    Y_test,            # Test labels (one-hot encoded)\n","    verbose=1,         # Print progress bar for evaluation\n","    batch_size=batch_size  # Number of samples per evaluation step\n",")\n","\n","# Print evaluation results: loss and accuracy on test set\n","print(score)"],"metadata":{"id":"gLx2F55gUSFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_history(history) # plot loss curve"],"metadata":{"id":"YRiuseqsIPCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model):\n","    # ===========================\n","    # Load the best-trained model weights\n","    # ===========================\n","    model.load_weights(filepath)  # Load weights saved during training from 'weights/weights.h5'\n","\n","    # ===========================\n","    # Predict labels for the test set\n","    # ===========================\n","    test_Y_hat = model.predict(X_test, batch_size=batch_size)\n","    # 'test_Y_hat' contains predicted probabilities for each class (one-hot-like format)\n","\n","    # ===========================\n","    # Compute confusion matrix and overall accuracy\n","    # ===========================\n","    cm, right, wrong = calculate_confusion_matrix(Y_test, test_Y_hat, classes)\n","    # cm: normalized confusion matrix\n","    # right: number of correct predictions\n","    # wrong: number of incorrect predictions\n","\n","    acc = round(1.0 * right / (right + wrong), 4)  # Overall accuracy (0-1 scale)\n","    print('Overall Accuracy: %.2f%s / (%d + %d)' % (100 * acc, '%', right, wrong))\n","    # Prints accuracy as a percentage and counts of correct/incorrect predictions\n","\n","    # ===========================\n","    # Plot confusion matrix\n","    # ===========================\n","    plot_confusion_matrix(\n","        cm,\n","        labels=['BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '64PSK',\n","                '4QAM', '8QAM', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM',\n","                '2FSK', '4FSK', '8FSK', '16FSK',\n","                '4PAM', '8PAM', '16PAM',\n","                'AM-DSB', 'AM-DSB-SC', 'AM-USB', 'AM-LSB', 'FM', 'PM'],\n","        save_filename='figure/total_confusion.png'\n","    )\n","    # Saves the confusion matrix as a PNG file for visualization\n","\n","    # ===========================\n","    # Calculate accuracy for each SNR and plot metrics vs SNR\n","    # ===========================\n","    calculate_acc_cm_each_snr(\n","        Y_test,          # True labels\n","        test_Y_hat,      # Predicted labels\n","        Z_test,          # SNR values for each test sample\n","        classes,         # List of modulation classes\n","        min_snr=-18      # Minimum SNR value to plot\n","    )\n","    # - Plots overall accuracy, precision, recall, and F1-score vs SNR"],"metadata":{"id":"X9BJfQoiq6_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model)  #Computes confusion matrices for different SNR levels"],"metadata":{"id":"laOlXt4Mrpuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# Record the end time of testing and calculate total testing duration\n","# ===========================\n","\n","TEE_PROPOSED = time.time()           # Capture the current time after test evaluation and prediction\n","TE_PROPOSED = TEE_PROPOSED - TES_PROPOSED  # Calculate total testing time in seconds"],"metadata":{"id":"1BBUAMOOxV7Q"},"execution_count":null,"outputs":[]}]}