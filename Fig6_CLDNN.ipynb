{"cells":[{"cell_type":"code","execution_count":null,"id":"6be5215c","metadata":{"id":"6be5215c"},"outputs":[],"source":["# ===========================\n","# Import necessary libraries\n","# ===========================\n","import os\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Dense, Conv1D, MaxPool1D, ReLU, Dropout, Softmax, concatenate, Conv2D\n","from keras.layers import LSTM, Permute, Reshape, ZeroPadding2D, Activation\n","from keras.utils.vis_utils import plot_model\n","\n","# ===========================\n","# Define a CLDNN-like model\n","# ===========================\n","def CLDNNLikeModel(weights=None,\n","                   input_shape1=[2,1024],\n","                   classes=26,\n","                   **kwargs):\n","    \"\"\"\n","    CLDNN-like model combining CNN + LSTM + Dense layers for modulation classification.\n","    Supports optional loading of pre-trained weights.\n","    \"\"\"\n","\n","    # Check if weights path exists if provided\n","    if weights is not None and not (os.path.exists(weights)):\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization), '\n","                         'or the path to the weights file to be loaded.')\n","\n","    dr = 0.5  # Dropout rate to reduce overfitting\n","\n","    # Input layer: shape = (channels, features) with an extra dimension for Conv2D\n","    input_x = Input(shape=(1, 2, 1024))\n","\n","    # Zero-padding to maintain dimensions after convolution\n","    input_x_padding = ZeroPadding2D((0, 2), data_format=\"channels_first\")(input_x)\n","\n","    # -------------------\n","    # Convolutional layers\n","    # -------------------\n","    layer11 = Conv2D(50, (1, 8), padding='valid', activation=\"relu\", name=\"conv11\",\n","                     kernel_initializer=\"glorot_uniform\", data_format=\"channels_first\")(input_x_padding)\n","    layer11 = Dropout(dr)(layer11)  # Dropout for regularization\n","\n","    layer11_padding = ZeroPadding2D((0, 2), data_format=\"channels_first\")(layer11)\n","    layer12 = Conv2D(50, (1, 8), padding=\"valid\", activation=\"relu\", name=\"conv12\",\n","                     kernel_initializer=\"glorot_uniform\", data_format=\"channels_first\")(layer11_padding)\n","    layer12 = Dropout(dr)(layer12)\n","\n","    layer12 = ZeroPadding2D((0, 2), data_format=\"channels_first\")(layer12)\n","    layer13 = Conv2D(50, (1, 8), padding='valid', activation=\"relu\", name=\"conv13\",\n","                     kernel_initializer=\"glorot_uniform\", data_format=\"channels_first\")(layer12)\n","    layer13 = Dropout(dr)(layer13)\n","\n","    # -------------------\n","    # Concatenate first and last conv layers for richer features\n","    # -------------------\n","    concat = keras.layers.concatenate([layer11, layer13])\n","    concat_size = list(np.shape(concat))\n","    input_dim = int(concat_size[-1] * concat_size[-2])  # Flatten feature dimensions\n","    timesteps = int(concat_size[-3])  # Number of timesteps for LSTM\n","\n","    # Reshape to (samples, timesteps, input_dim) for LSTM\n","    concat = Reshape((timesteps, input_dim))(concat)\n","\n","    # -------------------\n","    # LSTM layer for temporal feature extraction\n","    # -------------------\n","    lstm_out = LSTM(50, input_dim=input_dim, input_length=timesteps)(concat)\n","\n","    # -------------------\n","    # Dense layers for classification\n","    # -------------------\n","    layer_dense1 = Dense(256, activation='relu', kernel_initializer='he_normal', name=\"dense1\")(lstm_out)\n","    layer_dropout = Dropout(dr)(layer_dense1)\n","    layer_dense2 = Dense(26, kernel_initializer='he_normal', name=\"dense2\")(layer_dropout)\n","\n","    # Softmax activation to output probabilities for each class\n","    layer_softmax = Activation('softmax')(layer_dense2)\n","    output = Reshape([26])(layer_softmax)  # Ensure output shape matches number of classes\n","\n","    # Build the model\n","    model = Model(inputs=input_x, outputs=output)\n","\n","    # Load pre-trained weights if provided\n","    if weights is not None:\n","        model.load_weights(weights)\n","\n","    return model\n","\n","\n","# ===========================\n","# Main execution: compile and inspect model\n","# ===========================\n","import keras\n","if __name__ == '__main__':\n","    # Instantiate the CLDNN-like model\n","    model = CLDNNLikeModel(None, input_shape=(2,1024), classes=24)\n","\n","    # Compile the model with categorical crossentropy loss and Adam optimizer\n","    adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n","                                 epsilon=None, decay=0.0, amsgrad=False)\n","    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)\n","\n","    # Print detailed information about the model\n","    print('Model layers:', model.layers)        # List of all layers\n","    print('Model config:', model.get_config())  # Configuration of the model\n","    print('Model summary:', model.summary())   # Summary of layers, output shapes, and parameters"]},{"cell_type":"code","execution_count":null,"id":"bab042c1","metadata":{"id":"bab042c1"},"outputs":[],"source":["import matplotlib\n","matplotlib.use('TkAgg')  # Use TkAgg backend for interactive plotting\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import csv\n","import itertools\n","from sklearn.metrics import precision_recall_fscore_support, mean_squared_error, mean_absolute_error, r2_score\n","\n","# ===========================\n","# Function: Plot confusion matrix\n","# ===========================\n","def plot_confusion_matrix(cm, title='', cmap=plt.get_cmap(\"Blues\"), labels=[], save_filename=None):\n","    \"\"\"\n","    Visualizes a confusion matrix with class labels and optional saving as PDF.\n","    \"\"\"\n","    plt.figure(figsize=(10, 7))\n","    plt.imshow(cm*100, interpolation='nearest', cmap=cmap)  # Multiply by 100 to show in %\n","    plt.title(title, fontsize=10)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(labels))\n","    plt.xticks(tick_marks, labels, rotation=90, size=12)\n","    plt.yticks(tick_marks, labels, size=12)\n","\n","    # Annotate each cell with the value\n","    for i in range(len(tick_marks)):\n","        for j in range(len(tick_marks)):\n","            text_color = 'darkorange' if i == j else 'black'\n","            text = plt.text(j, i, int(np.around(cm[i,j]*100)), ha=\"center\", va=\"center\", fontsize=10, color=text_color)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label', fontdict={'size':16})\n","    plt.xlabel('Predicted label', fontdict={'size':16})\n","\n","    # Save figure if filename provided\n","    if save_filename is not None:\n","        plt.savefig(save_filename, format='pdf', dpi=1200, bbox_inches='tight')\n","    plt.close()\n","\n","# ===========================\n","# Function: Calculate confusion matrix\n","# ===========================\n","def calculate_confusion_matrix(Y, Y_hat, classes):\n","    \"\"\"\n","    Computes the normalized confusion matrix and counts of correct and incorrect predictions.\n","    \"\"\"\n","    n_classes = len(classes)\n","    conf = np.zeros([n_classes, n_classes])\n","    confnorm = np.zeros([n_classes, n_classes])\n","\n","    for k in range(Y.shape[0]):\n","        i = list(Y[k,:]).index(1)           # True class index\n","        j = int(np.argmax(Y_hat[k,:]))      # Predicted class index\n","        conf[i,j] += 1\n","\n","    # Normalize each row to sum to 1\n","    for i in range(n_classes):\n","        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n","\n","    right = np.sum(np.diag(conf))           # Correct predictions\n","    wrong = np.sum(conf) - right            # Incorrect predictions\n","    return confnorm, right, wrong\n","\n","# ===========================\n","# Function: Calculate per-class accuracy from confusion matrix\n","# ===========================\n","def calculate_acc_at1snr_from_cm(cm):\n","    \"\"\"\n","    Returns the recognition accuracy per class from the confusion matrix.\n","    \"\"\"\n","    return np.round(np.diag(cm) / np.sum(cm, axis=1), 3)\n","\n","# ===========================\n","# Function: Compute standard metrics\n","# ===========================\n","def calculate_metrics(Y, Y_hat):\n","    \"\"\"\n","    Computes accuracy, precision, recall, F1-score, MSE, MAE, and R2 score.\n","    \"\"\"\n","    Y_true = np.argmax(Y, axis=1)\n","    Y_pred = np.argmax(Y_hat, axis=1)\n","\n","    accuracy = accuracy_score(Y_true, Y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='weighted')\n","\n","    mse = mean_squared_error(Y_true, Y_pred)\n","    mae = mean_absolute_error(Y_true, Y_pred)\n","    r2 = r2_score(Y_true, Y_pred)\n","\n","    return accuracy, precision, recall, f1\n","\n","# ===========================\n","# Function: Calculate metrics per SNR and optionally plot\n","# ===========================\n","def calculate_acc_cm_each_snr(Y, Y_hat, Z, classes=None, save_figure=True, min_snr=0):\n","    \"\"\"\n","    Computes accuracy, precision, recall, and F1-score for each SNR level.\n","    Optionally generates plots of metrics vs SNR.\n","    \"\"\"\n","    Z_array = Z[:, 0]  # Extract SNR values\n","    snrs = sorted(list(set(Z_array)))  # Unique SNR levels\n","    acc_mod_snr = np.zeros((len(classes), len(snrs)))\n","\n","    metrics = {\n","        'accuracy': [],\n","        'precision': [],\n","        'recall': [],\n","        'f1': []\n","    }\n","\n","    # Loop over each SNR and compute metrics\n","    for snr in snrs:\n","        Y_snr = Y[np.where(Z_array == snr)]\n","        Y_hat_snr = Y_hat[np.where(Z_array == snr)]\n","\n","        accuracy, precision, recall, f1 = calculate_metrics(Y_snr, Y_hat_snr)\n","        metrics['accuracy'].append(accuracy)\n","        metrics['precision'].append(precision)\n","        metrics['recall'].append(recall)\n","        metrics['f1'].append(f1)\n","\n","    # Plot metrics vs SNR\n","    for metric, values in metrics.items():\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(snrs, values, label=metric)\n","        for x, y in zip(snrs, values):\n","            plt.text(x, y, round(y, 3), ha='center', va='bottom', fontsize=8)\n","        plt.xlabel(\"Signal to Noise Ratio (dB)\")\n","        plt.ylabel(metric.capitalize())\n","        plt.title(f\"{metric.capitalize()} vs SNR\")\n","        plt.legend()\n","        plt.grid()\n","        plt.savefig(f'figure/{metric}_vs_snr.pdf', format='pdf', dpi=1200, bbox_inches='tight')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"4f124ed2","metadata":{"id":"4f124ed2"},"outputs":[],"source":["import os, random\n","\n","# Set Keras backend to TensorFlow\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Uncomment if you want to select a specific GPU (here GPU 0)\n","# os.environ[\"THEANO_FLAGS\"]  = \"device=gpu%d\"%(0)\n","\n","# Make only GPU 0 visible to TensorFlow/Keras\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","# matplotlib.use('Tkagg')  # Optional: Use TkAgg backend for interactive plotting\n","import matplotlib.pyplot as plt\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","import pickle, random, sys, h5py\n","\n","import keras\n","import keras.backend as K\n","from keras.callbacks import LearningRateScheduler, TensorBoard\n","from keras.regularizers import *\n","from keras.optimizers import Adam\n","from keras.models import model_from_json\n","from keras.utils.np_utils import to_categorical\n","\n","# ===========================\n","# Define modulation classes\n","# ===========================\n","# This list represents all modulation schemes in the HisarMod2019.1 dataset\n","classes = ['BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '64PSK',\n","           '4QAM', '8QAM', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM',\n","           '2FSK', '4FSK', '8FSK', '16FSK',\n","           '4PAM', '8PAM', '16PAM',\n","           'AM-DSB', 'AM-DSB-SC', 'AM-USB', 'AM-LSB',\n","           'FM', 'PM']\n","\n","# Each element in the list is used as a target class label when converting labels\n","# to one-hot vectors using `to_categorical()` during training and evaluation."]},{"cell_type":"code","execution_count":null,"id":"5158e821","metadata":{"id":"5158e821"},"outputs":[],"source":["# ===========================\n","# Load training data\n","# ===========================\n","data1 = h5py.File('Dataset/HisarMod2019.1/Train/train.mat','r')  # Open .mat file in read mode\n","train = data1['data_save'][:]                                   # Extract dataset from HDF5 group 'data_save'\n","\n","# Swap axes to match model input shape\n","# Original shape might be (num_samples, 2, 1024) or (1024, 2, num_samples)\n","# After swapaxes(0,2), shape becomes (num_samples, 2, 1024)\n","train = train.swapaxes(0,2)\n","\n","# ===========================\n","# Load test data\n","# ===========================\n","data2 = h5py.File('Dataset/HisarMod2019.1/Test/test.mat','r')\n","test = data2['data_save'][:]\n","test = test.swapaxes(0,2)\n","\n","# ===========================\n","# Add channel dimension\n","# ===========================\n","# Conv2D layers expect input of shape (samples, height, width, channels)\n","train = np.expand_dims(train, axis=3)  # Add a channel dimension: shape -> (num_samples, 2, 1024, 1)\n","test = np.expand_dims(test, axis=3)"]},{"cell_type":"code","execution_count":null,"id":"2a4bc948","metadata":{"id":"2a4bc948"},"outputs":[],"source":["# ===========================\n","# Load and preprocess labels\n","# ===========================\n","\n","# Load training labels from CSV file\n","train_labels = pd.read_csv('Dataset/HisarMod2019.1/Train/train_labels1.csv', header=None)\n","train_labels = np.array(train_labels)  # Convert DataFrame to NumPy array\n","\n","# Convert integer labels to one-hot encoding\n","# This is required for categorical cross-entropy loss in Keras\n","train_labels = to_categorical(train_labels, num_classes=None)\n","\n","\n","# Load test labels from CSV file\n","test_labels = pd.read_csv('Dataset/HisarMod2019.1/Test/test_labels1.csv', header=None)\n","test_labels = np.array(test_labels)    # Convert DataFrame to NumPy array\n","test_labels = to_categorical(test_labels, num_classes=None)  # One-hot encoding\n","\n","\n","# ===========================\n","# Load and preprocess SNR values\n","# ===========================\n","\n","# Load training SNR values from CSV\n","# Each row corresponds to the SNR of a training sample\n","train_snr = pd.read_csv('Dataset/HisarMod2019.1/Train/train_snr.csv', header=None)\n","train_snr = np.array(train_snr)  # Convert to NumPy array for easier indexing\n","\n","# Load test SNR values from CSV\n","# Each row corresponds to the SNR of a test sample\n","test_snr = pd.read_csv('Dataset/HisarMod2019.1/Test/test_snr.csv', header=None)\n","test_snr = np.array(test_snr)    # Convert to NumPy array"]},{"cell_type":"code","execution_count":null,"id":"1b900716","metadata":{"id":"1b900716"},"outputs":[],"source":["# ===========================\n","# Split dataset into training and validation sets\n","# ===========================\n","\n","# Number of total examples in the training dataset\n","n_examples = train.shape[0]  # train has shape [N, 1024, 2]\n","\n","# Number of samples for training and validation\n","n_train = int(n_examples * 0.8)  # 80% for training\n","n_val = int(n_examples * 0.2)    # 20% for validation\n","\n","# Randomly select indices for training samples\n","train_idx = list(np.random.choice(range(0, n_examples), size=n_train, replace=False))\n","\n","# Remaining indices will be used for validation\n","val_idx = list(set(range(0, n_examples)) - set(train_idx))\n","\n","# Shuffle indices to ensure random ordering\n","np.random.shuffle(train_idx)\n","np.random.shuffle(val_idx)\n","\n","# ===========================\n","# Create datasets using the indices\n","# ===========================\n","\n","# Training data and labels\n","X_train = train[train_idx]         # Shape: [n_train, 1024, 2]\n","Y_train = train_labels[train_idx]  # Shape: [n_train, num_classes]\n","\n","# Validation data and labels\n","X_val = train[val_idx]             # Shape: [n_val, 1024, 2]\n","Y_val = train_labels[val_idx]      # Shape: [n_val, num_classes]\n","\n","# Test data and labels (use full test set)\n","X_test = test                     # Shape: [num_test_samples, 1024, 2]\n","Y_test = test_labels               # Shape: [num_test_samples, num_classes]\n","\n","# Test SNR values corresponding to each test sample\n","Z_test = test_snr                  # Shape: [num_test_samples, 1] or similar"]},{"cell_type":"code","execution_count":null,"id":"3f7e2179","metadata":{"id":"3f7e2179"},"outputs":[],"source":["# Set up some params\n","nb_epoch = 200     # number of epochs to train on\n","batch_size = 300  # training batch size"]},{"cell_type":"code","execution_count":null,"id":"f28ed861","metadata":{"id":"f28ed861"},"outputs":[],"source":["# ===========================\n","# Initialize the CLDNN-like model\n","# ===========================\n","# `CLDNNLikeModel` is a custom model combining CNN and LSTM layers\n","# Input shape corresponds to (channels, time_steps) = [2, 1024]\n","model = CLDNNLikeModel(None, input_shape=[2,1024])\n","\n","# ===========================\n","# Compile the model\n","# ===========================\n","# Loss: categorical_crossentropy for multi-class classification\n","# Metrics: accuracy to monitor performance\n","# Optimizer: Adam for adaptive gradient updates\n","model.compile(\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy'],\n","    optimizer='Adam'\n",")\n","\n","# ===========================\n","# Visualize the model architecture\n","# ===========================\n","# `plot_model` saves a diagram of the model layers to 'model_CLDNN.png'\n","# `show_shapes=True` includes the shape of tensors at each layer\n","plot_model(model, to_file='model_CLDNN.png', show_shapes=True)\n","model.summary()\n","# After this step, you can open 'model_CLDNN.png' to inspect the architecture,\n","# which is useful for debugging or reporting.\n"]},{"cell_type":"code","execution_count":null,"id":"f129012d","metadata":{"id":"f129012d"},"outputs":[],"source":["# ===========================\n","# Define file path to save best model weights\n","# ===========================\n","# During training, the best weights based on validation loss will be saved here\n","filepath = 'weights/CLDNN.h5'\n","\n","import time\n","\n","# ===========================\n","# Record start time for training\n","# ===========================\n","TRS_CLDNN = time.time()\n","\n","# ===========================\n","# Train the CLDNN model\n","# ===========================\n","history = model.fit(\n","    X_train,           # Training input data\n","    Y_train,           # Training labels (one-hot encoded)\n","    batch_size=batch_size,  # Number of samples per gradient update\n","    epochs=nb_epoch,        # Total number of training epochs\n","    verbose=2,              # 0 = silent, 1 = progress bar, 2 = one line per epoch\n","    validation_data=(X_val, Y_val),  # Validation set to monitor overfitting\n","    callbacks=[             # Callbacks to improve training and save best weights\n","        # Save the model weights when validation loss improves\n","        keras.callbacks.ModelCheckpoint(\n","            filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'\n","        ),\n","        # Reduce learning rate by factor 0.5 if validation loss plateaus\n","        keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_loss', factor=0.5, verbose=1, patince=5, min_lr=1e-7\n","        ),\n","        # Stop training early if validation loss doesn't improve after 50 epochs\n","        keras.callbacks.EarlyStopping(\n","            monitor='val_loss', patience=50, verbose=1, mode='auto'\n","        ),\n","        # Optional: TensorBoard callback for visualizing training metrics\n","        # keras.callbacks.TensorBoard(log_dir='./logs/', histogram_freq=1, write_graph=False,\n","        #                             write_grads=1, write_images=False, update_freq='epoch')\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"dac8986f","metadata":{"id":"dac8986f"},"outputs":[],"source":["# ===========================\n","# Record end time after training\n","# ===========================\n","TRE_CLDNN = time.time()\n","\n","# ===========================\n","# Calculate total training time\n","# ===========================\n","# T_CLDNN stores the elapsed time in seconds that the model took to train\n","T_CLDNN = TRE_CLDNN - TRS_CLDNN\n","\n","# You can print T_CLDNN to see the total training duration\n","# print(\"Total training time for CLDNN: {:.2f} seconds\".format(T_CLDNN))"]},{"cell_type":"code","execution_count":null,"id":"22e43739","metadata":{"id":"22e43739"},"outputs":[],"source":["# ===========================\n","# Start timing the evaluation process\n","# ===========================\n","TES_CLDNN = time.time()\n","\n","# ===========================\n","# Evaluate the trained CLDNN model on the test set\n","# ===========================\n","# X_test : test input data\n","# Y_test : corresponding test labels (one-hot encoded)\n","# verbose=1 : shows a progress bar during evaluation\n","# batch_size : number of samples processed at a time during evaluation\n","score = model.evaluate(X_test, Y_test, verbose=1, batch_size=batch_size)\n","\n","# ===========================\n","# Print the evaluation metrics\n","# ===========================\n","# score[0] : test loss (categorical crossentropy)\n","# score[1] : test accuracy\n","print(score)\n","\n","# ===========================\n","# This provides a quick summary of the model's performance on unseen data\n","# ==========================="]},{"cell_type":"code","source":["calculate_acc_cm_each_snr(Y_test, test_Y_hat, Z_test, classes, min_snr=-18) #Calculate accuracy"],"metadata":{"id":"QPqAbyD47JjH"},"id":"QPqAbyD47JjH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"200d0dde","metadata":{"id":"200d0dde"},"outputs":[],"source":["# ===========================\n","# Record end time after model evaluation\n","# ===========================\n","TEE_CLDNN = time.time()\n","\n","# ===========================\n","# Calculate total evaluation time\n","# ===========================\n","# T_CLDNN stores the elapsed time in seconds that the model took to evaluate on the test set\n","T_CLDNN = TEE_CLDNN - TES_CLDNN\n","\n","# You can print T_CLDNN to see the total evaluation duration\n","# print(\"Total evaluation time for CLDNN: {:.2f} seconds\".format(T_CLDNN))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}